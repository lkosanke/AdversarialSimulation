---
title: "Methods & Results"
author: Valentin Kriegmair
format:
    pdf:
        fontsize: 12pt
        linestretch: 1.5
        geometry: "left=25mm, right=20mm, top=20mm, bottom=20mm"
        classoption: oneside
        papersize: a4
        header-includes:
           - \usepackage{float}
           - \floatplacement{table}{H}
           - \usepackage{tikz}
           - \usetikzlibrary{arrows.meta, positioning, calc}
           - \usepackage{geometry}
           - \geometry{margin=1in}
fontsize: 12pt
engine: knitr
bibliography: ../../bibliography.bib
---

```{r, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, echo = FALSE)
```

```{r}
library(dplyr)
library(tidyr)
library(kableExtra)
library(ggplot2)
library(data.table)
summary_1 <- readRDS("../simulation/results/summary_study1.rds")
summary_2 <- readRDS("../simulation/results/summary_study2.rds")
```

# Methods

The following description of the simulation studies is based on the established structure for simulation studies to coalign with the other conducted studies to facilitate a potential collaboration.

**Aims, objectives and research questions**

Both studies aimed to evaluate the performance of vanilla SEM compared to global SAM (gSAM), local SAM with maximum likelihood (lSAM-ML), and local SAM with unweighted least squares (lSAM-ULS) under various conditions. The two research questions we established prior to conducting the studies served as general basis for both studies

**Population Models and Data Generation Mechanisms**

**Study 1:**

Data were generated based on a 5-factor population structural model with 3 indicators for each factor. Four different models were simulated (see figure 1-4)

-   Model 1.1: Correctly specified model.
-   Model 1.2: Misspecified with cross-loadings ignored in the estimation model.
-   Model 1.3: Misspecified with wrong direction of path and omitted correlated residuals.
-   Model 1.4: Misspecified with multiple omitted structural paths.

For all models, the population-level values of the structural parameters were set to 0.1. Indicator reliability levels were manipulated with factor loadings set at low (λ = 0.3), moderate (λ = 0.5), or high (λ = 0.7).

::: {layout="[[40,-20,40],[40,-20,40]]"}
![Model 1.1](figures/model1_1.tex){#fig-model1} Figure 1: Note. Model 1.1: Error terms are not explicitly shown in the figure.

![Model 1.2](figures/model1_2.tex){#fig-model2} Figure 2: Note. Model 1.2: Error terms are not explicitly shown in the figure.

![Model 1.3](figures/model1_3.tex){#fig-model3} Figure 3: Note. Model 1.3: Error terms are not explicitly shown in the figure.

![Model 1.4](figures/model1_4.tex){#fig-model4} Figure 4: Note. Model 1.4: Error terms are not explicitly shown in the figure.
:::

**Study 2:**

Data were generated based on a 5-factor population structural model with 3 indicators for each factor, similar to Study 1, but with the additional condition of varying variance explained (R\^2) by the endogenous factors was set at low (R\^2 = 0.1) or medium (R\^2 = 0.4), so that the regression weights were either between 0.183 and 0.224 (low) or between 0.365 and 0.447 (medium). Note however that the computation of this was a simplification and does not acurately result in said R\^2 values. The aim here was only generally to modulate between lower and higher regression weights. Misspecifications included omitting a residual covariance and a factor cross-loading in either the exogenous or endogenous part of the model, or both (see figure 5-6).

::: {layout="[[40,-20,40], [100]]"}
![Model 2.1](figures/model2_1.tex) Figure 5: Note. Model 2.1: Error terms are not explicitly shown in the figure.

![Model 2.2](figures/model2_2.tex) Figure 6: Note. Model 2.2: Error terms are not explicitly shown in the figure Orange lines represent misspecifications in the endogenous part of the model. Green lines represent misspecifications in the exogenous part of the model. These types of misspecifications result in different realisations of model 2.2 when they are modulated as factors in study 2 but are subsumed under one model here.
:::

**Experimental Design of simulation procedures**

**Study 1:** The study varied three main conditions: - Sample sizes: small (N = 100), moderate (N = 400), and large (N = 6400). - Indicator reliability: low (λ = 0.3), moderate (λ = 0.5), high (λ = 0.7). - Model specifications: correctly specified model and misspecified models (1.2, 1.3, and 1.4).

**Study 2:** The study varied five main conditions: - Sample sizes: small (N = 100), medium (N = 400), and large (N = 6400). - Variance explained by endogenous factors: low (R\^2 = 0.1) and medium (R\^2 = 0.4). - Indicator reliability: low (λ = 0.3), moderate (λ = 0.5), and high (λ = 0.7). - Model misspecifications: varying the population model by omitting a residual covariance and a factor cross-loading in different parts of the model. - Number of measurement blocks: separate measurement model per latent variable (b = 5) and joint measurement model for all exogenous variables (b = 3).

**Method Selection**

**Study 1 and Study 2:** The performance of four estimation methods was compared: - Vanilla SEM. - Global SAM (gSAM). - Local SAM with maximum likelihood (lSAM-ML). - Local SAM with unweighted least squares (lSAM-ULS).

**Performance Measures**

The following performance measures were captured: - Convergence rates. - Empirical relative biases. - Empirical coverage levels of 95% confidence intervals (CIs). - Root Mean Squared Errors (RMSE).

**Software**

All analyses were conducted in @r_core_team_r_2023. See https://github.com/valentinkm/AdversarialSimulation for more details.

**Analysis and Interpretation plan**

The results were interpreted by descriptively comparing the performance measures (bias, MSE, convergence rates) of the different estimation methods under varying sample sizes, indicator reliability levels, and model misspecifications.

# Results

In the following the main patterns and trends of the results are reported and only examplary of select conditions are shown for each study to illustrate the main findings. The full results can be found in the supplementary materials.

## Study 1

### Convergence Rate

For moderate and large sample sizes (N = 400, 6400), all methods achieve a 100% convergence rate. For a small sample size (N = 100), SEM shows lower convergence rates, particularly at lower reliability levels (0.3), while GSAM, SAM-ML, and SAM-ULS maintain high convergence rates. Higher reliability levels (0.5 and 0.7) generally correspond to higher convergence rates for all methods. SEM exhibits notable drops in convergence at the lowest reliability (0.3), especially under cross loadings (1.2) and structural (1.4) misspecification. GSAM, SAM-ML, and SAM-ULS consistently show robust performance across all conditions. The omitted cross loadings (1.2) are particularly challenging for SEM with a convergence rate of 72.34% at the smallest sample size and lowest reliability. These results suggest that GSAM, SAM-ML, and SAM-ULS are more reliable for achieving model convergence, especially in scenarios with smaller sample sizes and lower reliability than vanilla SEM estimation.

```{r}
create_convergence_table_1 <- function(table_number, custom_caption, model_type_value) {
  summary_1 %>%
    filter(model_type == model_type_value) %>%
    select(N, reliability, method, ConvergenceRate) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `Method` = method,
      `Convergence Rate` = ConvergenceRate
    ) %>%
    mutate(
      `Convergence Rate` = round(`Convergence Rate`, 3),
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Convergence Rate`) %>%
    kbl(caption = custom_caption,
        format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the tables for each model type in Study 1 with custom captions
convergence_table_1_1 <- create_convergence_table_1(0, "Convergence Rates under correct model specification (Model 1.1)", "1.1")
#convergence_table_1_1

convergence_table_1_2 <- create_convergence_table_1(0, "Convergence Rates under omitted Cross Loadings (Model 1.2)", "1.2")
#convergence_table_1_2

convergence_table_1_3 <- create_convergence_table_1(1, "Convergence Rates under omitted Correlated Residuals and structural misspecifcation (Model 1.3)", "1.3")
#convergence_table_1_3

convergence_table_1_4 <- create_convergence_table_1(0, "Convergence Rates under Structural Misspecification (Model 1.4)", "1.4")
#convergence_table_1_4

```

### Average Relative Biases (in Percentages)

Here this study diverges from the original of @rosseel_structural_2022 by focusing exclusively on the average relative bias under a single type of correctly specified model (model 1), which does not include any cross loadings or correlated residuals. This simplification, allows to concentrate on the core advantage of SAM over traditional SEM: its more robust estimation of structural parameters, especially when measurement models are misspecified. By narrowing the scope, we aim to highlight SAM's robustness and its potential as a superior alternative in scenarios where traditional SEM may falter. For the correct model (1.1), SEM shows higher biases at smaller sample sizes and lower reliability, while GSAM, SAM-ML, and SAM-ULS exhibit lower biases. Under omitted cross loadings (1.2), all methods show substantial biases, especially SEM, at smaller sample sizes and lower reliability. In the case of omitted correlated residuals (1.3), GSAM, SAM-ML, and SAM-ULS show more negative biases compared to SEM. The structuraly misspecified model (1.4) showed SEM with the highest biases at lower sample sizes and reliability, while GSAM, SAM-ML, and SAM-ULS have relatively lower and similar biases. Overall, GSAM, SAM-ML, and SAM-ULS are more robust with lower biases compared to SEM, especially under challenging conditions with smaller sample sizes and lower reliability. Table 2 shows average relative bias percentages for different methods (SEM, GSAM, SAM-ML, SAM-ULS) under varying model specifications and sample sizes under under omitted correlated residuals (model 1.2)

```{r}
create_bias_table <- function(table_number, custom_caption, model_type_value) {
  summary_1 %>%
    filter(model_type == model_type_value) %>%
    select(N, reliability, method, MeanRelativeBias) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `Method` = method,
      `Average Relative Bias (%)` = MeanRelativeBias
    ) %>%
    mutate(
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS"),
      `Average Relative Bias (%)` = round(`Average Relative Bias (%)` * 100, 3)
    ) %>%
    pivot_wider(names_from = Method, values_from = `Average Relative Bias (%)`) %>%
    kbl(caption = custom_caption,
        format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the bias tables for each model type in Study 1 with custom captions
bias_table_1_1 <- create_bias_table(1, "Average Relative Biases (in Percentages) under correct model specification (Model 1.1)", "1.1")
#bias_table_1_1

bias_table_1_2 <- create_bias_table(2, "Average Relative Biases (in Percentages) under omitted Cross Loadings (Model 1.2)", "1.2")
bias_table_1_2

bias_table_1_3 <- create_bias_table(3, "Average Relative Biases (in Percentages) under omitted Correlated Residuals (Model 1.3)", "1.3")
#bias_table_1_3

bias_table_1_4 <- create_bias_table(4, "Average Relative Biases (in Percentages) under Structural Misspecification (Model 1.4)", "1.4")
#bias_table_1_4

```

### Coverage

For the correct (1.1) model, SEM shows undercoverage at smaller sample sizes and lower reliability, such as -1.839% at N=100 and reliability 0.3, indicating narrow CIs and less reliable parameter estimates. GSAM, SAM-ML, and SAM-ULS tend to exhibit slight overcoverage, with values like 3.104%, 3.151%, and 2.701%, respectively, indicating more conservative estimates. Under the cross loadings (1.2) model, all methods display substantial undercoverage, particularly SEM, which shows values as low as -65.231% at N=6400 and reliability 0.3. This highlights the challenge of this model mis-specification. Correlated residuals (Model 1.3) model reveal consistent undercoverage across all methods, with SEM showing the highest negative biases, such as -87.016% at N=6400 and reliability 0.3. The structural (1.4) model indicates that SEM suffers from undercoverage at lower sample sizes and reliability, with -1.471% at N=100 and reliability 0.3, while GSAM, SAM-ML, and SAM-ULS perform better with relatively lower and more consistent biases. For example, GSAM shows 3.293% at the same conditions. Overall, GSAM, SAM-ML, and SAM-ULS demonstrate better empirical coverage than SEM, but significant biases still exist under certain misspecifications. Table 3 show the average difference between empirical coverage levels of the 95% confidence intervals (CIs) and their nominal level (95%) using each method for different reliability values and sample sizes under omitted cross loadings (model 1.2)

```{r}
create_coverage_diff_table <- function(table_number, custom_caption, model_type_value) {
  summary_1 %>%
    filter(model_type == model_type_value) %>%
    select(N, reliability, method, MeanCoverage) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `Method` = method,
      `Average Coverage Difference (%)` = MeanCoverage
    ) %>%
    mutate(
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS"),
      `Average Coverage Difference (%)` = round((`Average Coverage Difference (%)` - 0.95) * 100, 3)
    ) %>%
    pivot_wider(names_from = Method, values_from = `Average Coverage Difference (%)`) %>%
    kbl(caption = custom_caption,
        format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the coverage difference tables for each model type in Study 1 with custom captions
coverage_diff_table_1_1 <- create_coverage_diff_table(1, "Average Coverage Difference (in Percentages) under correct model specification (Model 1.1)", "1.1")
#coverage_diff_table_1_1

coverage_diff_table_1_2 <- create_coverage_diff_table(2, "Average Coverage Difference (in Percentages) under omitted Cross Loadings (Model 1.2)", "1.2")
coverage_diff_table_1_2

coverage_diff_table_1_3 <- create_coverage_diff_table(3, "Average Coverage Difference (in Percentages) under omitted Correlated Residuals (Model 1.3)", "1.3")
#coverage_diff_table_1_3

coverage_diff_table_1_4 <- create_coverage_diff_table(4, "Average Coverage Difference (in Percentages) under Structural Misspecification (Model 1.4)", "1.4")
#coverage_diff_table_1_4
```

### RMSE

For the correct model (1.1), SEM generally shows higher RMSE values at smaller sample sizes and lower reliability, such as 2.643 at $N = 100$ and reliability = 0.3, while GSAM, SAM-ML, and SAM-ULS demonstrate lower RMSE values. As the sample size increases, RMSE values for all methods become more similar. Under omitted cross loadings (1.2), SEM shows significantly higher RMSE values, such as 6.710 at $N = 100$ and reliability = 0.3, while GSAM, SAM-ML, and SAM-ULS perform better but still show increased RMSE values. In the case of omitted correlated residuals (1.3), SEM does not consistently have the highest RMSE values; for instance, at $N = 100$ and reliability = 0.3, SEM has an RMSE of 1.546 compared to GSAM, SAM-ML, and SAM-ULS with RMSEs around 1.318 to 1.350. The structural model (1.4) shows SEM with higher RMSE values at lower sample sizes and reliability, such as 3.476 at $N = 100$ and reliability = 0.3, while GSAM, SAM-ML, and SAM-ULS demonstrate lower RMSE values. Overall, GSAM, SAM-ML, and SAM-ULS are more robust with lower RMSE values compared to SEM, especially under challenging conditions with smaller sample sizes and lower reliability. All methods show improved performance with larger sample sizes and higher reliability. Overall, GSAM, SAM-ML, and SAM-ULS demonstrate better performance with lower RMSE values compared to SEM, particularly in challenging conditions with smaller sample sizes and lower reliability. However, all methods show increased RMSE values under certain model misspecifications.

```{r}
create_rmse_table <- function(table_number, custom_caption, model_type_value) {
  summary_1 %>%
    filter(model_type == model_type_value) %>%
    select(N, reliability, method, MeanRelativeRMSE) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `Method` = method,
      `Average RMSE` = MeanRelativeRMSE
    ) %>%
    mutate(
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS"),
      `Average RMSE` = round(`Average RMSE`, 3)
    ) %>%
    pivot_wider(names_from = Method, values_from = `Average RMSE`) %>%
    kbl(caption = custom_caption,
        format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the RMSE tables for each model type in Study 1 with custom captions
rmse_table_1_1 <- create_rmse_table(1, "Average RMSE under correct model specification (Model 1.1)", "1.1")
#rmse_table_1_1

rmse_table_1_2 <- create_rmse_table(2, "Average RMSE under omitted Cross Loadings (Model 1.2)", "1.2")
#rmse_table_1_2

rmse_table_1_3 <- create_rmse_table(3, "Average RMSE under omitted Correlated Residuals (Model 1.3)", "1.3")
#rmse_table_1_3

rmse_table_1_4 <- create_rmse_table(4, "Average RMSE under Structural Misspecification (Model 1.4)", "1.4")
#rmse_table_1_4
```

### Improper Solutions

Improper solutions occur when parameter estimates fall outside the boundary of the parameter space, such as when variances are estimated to be negative or correlations exceed an absolute value of one. These issues are more likely to arise in smaller sample sizes and can indicate problems with model estimation.

It is evident that SEM exhibited a significantly higher count of improper solutions compared to GSAM, SAM-ML, and SAM-ULS, especially in small sample sizes and lower reliability conditions. For example, under the correct model specification (model 1.1) with a sample size of 100 and reliability of 0.3, SEM has 27.30% improper solutions, whereas GSAM, SAM-ML, and SAM-ULS have negligible or no improper solutions. This pattern is consistent across all model mis-specifications, with SEM frequently encountering improper solutions in challenging conditions. In contrast, GSAM, SAM-ML, and SAM-ULS demonstrate robust performance with minimal improper solutions across all scenarios, highlighting their reliability in parameter estimation. Table 5 shows the count of improper solutions across different conditions under cross loadings (model 1.2).

```{r}
create_improper_solutions_table <- function(table_number, custom_caption, model_type_value) {
  summary_1 %>%
    filter(model_type == model_type_value) %>%
    select(N, reliability, method, ImproperSolutionsCount) %>%
    mutate(ImproperSolutionsCount = ImproperSolutionsCount / 10000 * 100) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `Improper Solutions Percentage` = ImproperSolutionsCount,
      `Method` = method
    ) %>%
    mutate(
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Improper Solutions Percentage`) %>%
    kbl(caption = custom_caption,
        format = "latex", booktabs = TRUE, digits = 2, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the improper solutions tables for each model type in Study 1 with custom captions
improper_solutions_table_1_1 <- create_improper_solutions_table(1, "Percentage of Improper Solutions under correct model specification (Model 1.1)", "1.1")
#improper_solutions_table_1_1

improper_solutions_table_1_2 <- create_improper_solutions_table(2, "Percentage of Improper Solutions under omitted Cross Loadings (Model 1.2)", "1.2")
improper_solutions_table_1_2

improper_solutions_table_1_3 <- create_improper_solutions_table(3, "Percentage of Improper Solutions under omitted Correlated Residuals (Model 1.3)", "1.3")
#improper_solutions_table_1_3

improper_solutions_table_1_4 <- create_improper_solutions_table(4, "Percentage of Improper Solutions under Structural Misspecification (Model 1.4)", "1.4")
#improper_solutions_table_1_4

```

## Study 2

### Convergence

For moderate and large sample sizes (N = 400, 6400), all methods achieve a 100% convergence rate across all conditions, indicating that sample size is a significant factor in ensuring model convergence. For a small sample size (N = 100), SEM shows lower convergence rates, particularly at lower reliability levels (0.3), while GSAM, SAM-ML, and SAM-ULS maintain high convergence rates. This suggests that the reliability of indicators has a considerable impact on the convergence of SEM, with higher reliability levels (0.5 and 0.7) generally corresponding to higher convergence rates for all methods. R-squared values (0.1 and 0.4) do not significantly impact the convergence rates across methods, as evidenced by consistent high convergence rates in all conditions except for SEM at the lowest reliability and sample size. Measurement block size (b = 3 or b = 5) has minimal impact on convergence rates, indicating that the number of measurement blocks does not substantially affect the ability of the methods to converge.enarios with smaller sample sizes and lower reliability than vanilla SEM estimation. Also under different model misspecifications SEM struggles more with convergence, particularly in scenarios with lower reliability and smaller sample sizes, while GSAM, SAM-ML, and SAM-ULS maintain high convergence rates across all misspecifications.

```{r}
create_convergence_table <- function(table_number, custom_caption, model_type_value) {
  summary_2 %>%
    filter(model_type == model_type_value) %>%
    select(N, reliability, R_squared, b, method, ConvergenceRate) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `R²` = R_squared,
      `Measurement Blocks` = b,
      `Method` = method,
      `Convergence Rate` = ConvergenceRate
    ) %>%
    mutate(
      `Convergence Rate` = round(`Convergence Rate`, 3),
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Convergence Rate`) %>%
    kbl(caption = custom_caption,
        format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the tables for each model type in Study 2 with custom captions
convergence_table_2_1 <- create_convergence_table(1, "Convergence Rates for correct model specification (Model 2.1)", "2.1")
#convergence_table_2_1

convergence_table_2_2_exo <- create_convergence_table(2, "Convergence Rates for Model Misspecifications (cross loadings and correlated residuals) in Exogenous Factors of the Model (Model 2.2-exo)", "2.2_exo")
#convergence_table_2_2_exo

convergence_table_2_2_endo <- create_convergence_table(3, "Convergence Rates for Model Misspecifications (cross loadings and correlated residuals) in Endogenous Factors of the Model (Model 2.2-endo)", "2.2_endo")
#convergence_table_2_2_endo

convergence_table_2_2_both <- create_convergence_table(4, "Convergence Rates for Model Misspecifications (cross loadings and correlated residuals) in Exogenous and Endogenous Factors of the Model (Model 2.2-both)", "2.2_both")
#convergence_table_2_2_both
```

### The Influence of Number of Measurement Blocks on Bias and RMSE

For low reliability ($\lambda = 0.3$) and small sample size ($N = 100$), the bias is generally less negative with 3 measurement blocks compared to 5. For example, in the correct model specification (Model 2.1), with $R^2 = 0.1$, the mean relative bias for SAM-ML is -1.40% with 3 blocks and -6.54% with 5 blocks, while for SAM-ULS it is 4.10% with 3 blocks and -0.80% with 5 blocks. This trend is consistent across most conditions, indicating that increasing the number of measurement blocks tends to increase the negative bias, especially for low reliability and small sample sizes. As reliability increases to $\lambda = 0.5$ and $\lambda = 0.7$, the biases for both SAM-ML and SAM-ULS become less negative with 3 measurement blocks. For instance, with $N = 400$, $\lambda = 0.7$, and $R^2 = 0.1$, SAM-ML shows a bias of 0.48% with 3 blocks compared to 0.34% with 5 blocks, and SAM-ULS shows a bias of 0.65% with 3 blocks compared to 0.52% with 5 blocks. For large sample sizes ($N = 6400$), the difference in biases between 3 and 5 blocks diminishes, suggesting that the impact of the number of measurement blocks is less pronounced with larger sample sizes. Overall, while 5 measurement blocks generally result in larger negative biases under low reliability and small sample sizes, this effect is reduced with higher reliability and larger sample sizes. Table 6 compares the mean relative biases for lSAM methods under different measurement blocks (3 separate blocks with one joint measurement model for the exogenous factors or 5 blocks with one for each factor) across varying levels of reliability and sample sizes misspecifcations in the exo- and endognous factors. For low reliability ($\lambda = 0.3$) and small sample size ($N = 100$), RMSE is generally lower with 5 measurement blocks compared to 3. For instance, with $R^2 = 0.1$, SAM-ML shows an RMSE of 1.005 with 5 blocks and 1.087 with 3 blocks, while SAM-ULS shows an RMSE of 1.187 with 5 blocks and 1.278 with 3 blocks. As reliability increases to $\lambda = 0.5$ and $\lambda = 0.7$, the RMSE differences between 3 and 5 blocks remain consistent, showing lower RMSE with 5 blocks. For larger sample sizes ($N = 400$ and $N = 6400$), the RMSE continues to be slightly lower with 5 measurement blocks but the difference diminishes. This pattern is consistent under the different model misspecifcations. Bias measures the average deviation of an estimator from the true value, indicating accuracy, while RMSE combines both bias and variance to reflect overall estimation error and reliability. Given that study 1 has shown that the primary benefit of SAM over SEM is most pronounced under conditions of low reliability and small sample sizes, the notable reduction in bias with 3 measurement blocks becomes critical. Although 5 measurement blocks generally result in lower RMSE, the difference in bias is more substantial in these specific conditions. Thus, in general and in the following interpretation of the results comparing all SAM methods (inlcuding gSAM and lSAM) to SEM we consider lSAM within the 3 measurement blocks condition only.

```{r}
create_relative_bias_table_lSAM <- function(table_number, custom_caption, model_type_value) {
  summary_2 %>%
    filter(model_type == model_type_value) %>%
    filter(method %in% c("lSAM_ML", "lSAM_ULS")) %>%  # Filter for lSAM methods
    select(N, reliability, R_squared, b, method, MeanRelativeBias) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `R²` = R_squared,
      `Measurement Blocks` = b,
      `Method` = method,
      `Mean Relative Bias` = MeanRelativeBias
    ) %>%
    mutate(
      `Mean Relative Bias` = round(`Mean Relative Bias` * 100, 2),  # Convert to percentage and round to 2 decimal places
      Method = recode(Method, 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Mean Relative Bias`) %>%
    kbl(caption = custom_caption,
        format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

relative_bias_table_lSAM_correct <- create_relative_bias_table_lSAM(5, "Mean Relative Bias for lSAM methods only under correct model specification (Model 2.1)", "2.1")
#relative_bias_table_lSAM_correct

relative_bias_table_lSAM_exo <- create_relative_bias_table_lSAM(6, "Mean Relative Bias for lSAM methods only under Model Misspecifications (cross loadings and correlated residuals) in Exogenous Factors of the Model (Model 2.2-exo)", "2.2_exo")
#relative_bias_table_lSAM_exo

relative_bias_table_lSAM_endo <- create_relative_bias_table_lSAM(7, "Mean Relative Bias for lSAM methods only under Model Misspecifications (cross loadings and correlated residuals) in Endogenous Factors of the Model (Model 2.2-endo)", "2.2_endo")
#relative_bias_table_lSAM_endo

relative_bias_table_lSAM_both <- create_relative_bias_table_lSAM(8, "Mean Relative Bias for lSAM methods only under Model Misspecifications (cross loadings and correlated residuals) in Exogenous and Endogenous Factors of the Model (Model 2.2-both)", "2.2_both")
relative_bias_table_lSAM_both
```

```{r}
# Function to create an RMSE table for lSAM methods only
create_rmse_diff_table_lSAM <- function(table_number, custom_caption, model_type_value) {
  summary_2 %>%
    filter(model_type == model_type_value) %>%
    filter(method %in% c("lSAM_ML", "lSAM_ULS")) %>%  # Filter for lSAM methods
    select(N, reliability, R_squared, b, method, MeanRelativeRMSE) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `R²` = R_squared,
      `Measurement Blocks` = b,
      `Method` = method,
      `Mean Relative RMSE` = MeanRelativeRMSE
    ) %>%
    mutate(
      `Mean Relative RMSE` = round(`Mean Relative RMSE`, 3),
      Method = recode(Method, 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Mean Relative RMSE`) %>%
    kbl(caption = custom_caption,
        format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}
# Create and display the lSAM tables with custom captions
rmse_diff_table_lSAM_5 <- create_rmse_diff_table_lSAM(13, "Mean Relative RMSE for lSAM methods only under correct model specification (Model 2.1)", "2.1")
#rmse_diff_table_lSAM_5

rmse_diff_table_lSAM_6 <- create_rmse_diff_table_lSAM(14, "Mean Relative RMSE for lSAM methods only under Model Misspecifications (cross loadings and correlated residuals) in Exogenous Factors of the Model (Model 2.2-exo)", "2.2_exo")
#rmse_diff_table_lSAM_6

rmse_diff_table_lSAM_7 <- create_rmse_diff_table_lSAM(15, "Mean Relative RMSE for lSAM methods only under Model Misspecifications (cross loadings and correlated residuals) in Endogenous Factors of the Model (Model 2.2-endo)", "2.2_endo")
#rmse_diff_table_lSAM_7

rmse_diff_table_lSAM_8 <- create_rmse_diff_table_lSAM(16, "Mean Relative RMSE for lSAM methods only under Model Misspecifications (cross loadings and correlated residuals) in Exogenous and Endogenous Factors of the Model (Model 2.2-both)", "2.2_both")
#rmse_diff_table_lSAM_8
```

### Average Relative Bias

For the correct specified model with low reliability ($\lambda$ = 0.3) and small sample size ($N = 100$), traditional SEM showed a substantially larger absolute value in average relative bias (14.12%) compared to all SAM methods. gSAM and lSAM-ML have small negative biases (-6.76% and -1.40%, respectively), indicating underestimation, while lSAM-ULS shows a positive bias (4.10%) in the most challenging condition. Higher regression weights (total variance in the endogenous factors explained by the exogenous factors) have no substantial impact on traditional SEM, while all SAM methods show some increase (e.g., for $N = 100$ and $\lambda$ = 0.3, SAM-ML changes from -1.40% with $R^2 = 0.1$ to 0.37% with $R^2 = 0.4$). Overall, for higher reliability and sample size, all methods achieve less biased estimates, and the difference between SEM and SAM becomes negligible. Under misspecification condition of correlated residuals and crossloadings in the exogenous part of the model (table 14) this pattern is consistent with an even greater positive bias for SEM in the challenging conditions (e.g. 61.99% for low reliability, small sample size and small regression coefficients) and similar negative biases for the SAM methods with SAM-ULS performing noticeably better overall and largely unaffected by varying $R^2$ values. However, under missspecifications (correlated residuals and cross-loadings) in the endogenous part of the model (Model 2.2-endo), this pattern differed. For low reliability ($\lambda = 0.3$) and small sample size ($N = 100$), traditional SEM exhibits relatively small biases compared to the SAM methods. For instance, with $R^2 = 0.1$, SEM has a mean relative bias of -0.35%, whereas gSAM, lSAM-ML, and lSAM-ULS show large negative biases (-24.56%, -16.81%, and -15.91%, respectively), indicating substantial underestimation. Under higher $R^2$, SEM's negative bias becomes more pronounced across all conditions (e.g., -11.39% with low reliability and sample size), while the biases for the SAM methods remain similarly large and negative irrespective of $R^2$. Another noticeable difference in this mis-specification condition is that with larger sample sizes ($N = 400$ and $N = 6400$), the biases for SEM become more pronounced, especially under low reliability conditions. For example, at $N = 400$, $\lambda = 0.3$, and $R^2 = 0.1$, SEM shows a bias of -16.16%, whereas SAM methods show slightly smaller negative biases (around -17% to -22%). This trend continues at $N = 6400$, where SEM's bias reaches -19.20% compared to the SAM methods' biases (around -18.38% to -24.37%). In this mis-specification condition, SAM-ULS does not show better performance overall. Finally, shown in table 9, under misspecifications in both the endogenous and the exogenous part of the model, the SAM methods also showed consistently similar negative biases, only Traditional SEM, similar to the exogenous mis-specification, showed a positive bias in low reliability and low sample size. For example, with $\lambda = 0.3$ and $N = 100$, SEM showed a bias of 9.39% when $R^2 = 0.1$ and 6.44% when $R^2 = 0.4$. However, in contrast to the condition with exogenous mis-specifications and consistent with the endogenous mis-specification conditions, the bias turned negative with increasing sample size under low reliability. This is evident as SEM's bias changes to -12.30% for $N = 400$, $\lambda = 0.3$, and $R^2 = 0.1$, and further to -16.73% for $N = 6400$, $\lambda = 0.3$, and $R^2 = 0.1$. This trend is additionally increased under high vs. low $R^2$ values. For example, at $N = 6400$ and $\lambda = 0.3$, the bias for SEM is -16.73 at $R^2 = 0.1$ and -20.17% at $R^2 = 0.4$.

```{r}
# Function to create a relative bias table for a specific model type with SEM performance included
create_relative_bias_table <- function(table_number, custom_caption, model_type_value) {
  summary_2 %>%
    filter(model_type == model_type_value) %>%
    filter(b == 3) %>%  # Filter for Measurement Blocks = 3
    select(N, reliability, R_squared, method, MeanRelativeBias) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `R²` = R_squared,
      `Method` = method,
      `Mean Relative Bias` = MeanRelativeBias
    ) %>%
    mutate(
      `Mean Relative Bias` = round(`Mean Relative Bias` * 100, 2),  # Convert to percentage and round to 2 decimal places
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Mean Relative Bias`) %>%
    kbl(caption = custom_caption,
    format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the tables for each model type with custom captions
relative_bias_table_correct <- create_relative_bias_table(1, "Mean Relative Bias for vanilla SEM and different SAM methods under correct model specification (Model 2.1)", "2.1")
# relative_bias_table_correct

relative_bias_table_exo <- create_relative_bias_table(2, "Mean Relative Bias for vanilla SEM and different SAM methods under Model Misspecifications (cross loadings and correlated residuals) in Exogenous Factors of the Model (Model 2.2-exo)", "2.2_exo")
# relative_bias_table_exo

relative_bias_table_endo <- create_relative_bias_table(3, "Mean Relative Bias for vanilla SEM and different SAM methods under Model Misspecifications (cross loadings and correlated residuals) in Endogenous Factors of the Model (Model 2.2-endo)", "2.2_endo")
#relative_bias_table_endo

relative_bias_table_both <- create_relative_bias_table(4, "Mean Relative Bias for vanilla SEM and different SAM methods under Model Misspecifications (cross loadings and correlated residuals) in Exogenous and Endogenous Factors of the Model (Model 2.2-both)", "2.2_both")
relative_bias_table_both
```

```{r}
# # Function to create a coverage table for a specific model type with differences to SEM
# create_coverage_diff_table <- function(model_type_label, model_type_value) {
#   sem_values <- summary_2 %>%
#     filter(model_type == model_type_value, method == "SEM") %>%
#     select(N, reliability, R_squared, b, MeanCoverage) %>%
#     rename(
#       Reliability = reliability,
#       R_Squared = R_squared,
#       `Measurement Blocks` = b,
#       SEM_Value = MeanCoverage
#     )
#   
#   summary_2 %>%
#     filter(model_type == model_type_value, method != "SEM") %>%
#     select(N, reliability, R_squared, b, method, MeanCoverage) %>%
#     left_join(sem_values, by = c("N", "reliability" = "Reliability", "R_squared" = "R_Squared", "b" = "Measurement Blocks")) %>%
#     mutate(Diff_to_SEM = MeanCoverage - SEM_Value) %>%
#     select(N, reliability, R_squared, b, method, Diff_to_SEM) %>%
#     rename(
#       Reliability = reliability,
#       R_Squared = R_squared,
#       `Measurement Blocks` = b,
#       Method = method,
#       `Difference to SEM` = Diff_to_SEM
#     ) %>%
#     mutate(
#       `Difference to SEM` = round(`Difference to SEM`, 3),
#       Method = recode(Method, 
#                       gSAM = "GSAM", 
#                       lSAM_ML = "SAM-ML", 
#                       lSAM_ULS = "SAM-ULS")
#     ) %>%
#     pivot_wider(names_from = Method, values_from = `Difference to SEM`) %>%
#     kbl(caption = paste("Difference in Mean Coverage to SEM for (Mis-)Specification (Model):", model_type_label),
#         format = "html", booktabs = TRUE) %>%
#     kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
# }
# 
# # Create and display the tables for each model type in Study 2
# coverage_diff_table_2_1 <- create_coverage_diff_table("2.1", "2.1")
# coverage_diff_table_2_1
# 
# coverage_diff_table_2_2_exo <- create_coverage_diff_table("2.2_exo", "2.2_exo")
# coverage_diff_table_2_2_exo
# 
# coverage_diff_table_2_2_endo <- create_coverage_diff_table("2.2_endo", "2.2_endo")
# coverage_diff_table_2_2_endo
# 
# coverage_diff_table_2_2_both <- create_coverage_diff_table("2.2_both", "2.2_both")
# coverage_diff_table_2_2_both
```

### RMSE

For the correctly specified model with low reliability ($\lambda = 0.3$) and small sample size ($N = 100$), traditional SEM showed a substantially larger RMSE (1.400) compared to all SAM methods. gSAM and SAM-ML had lower RMSEs (1.018 and 1.087, respectively), while SAM-ULS had a higher RMSE (1.278). As the regression weights ($R^2$) increased, RMSE improved for both SEM and SAM methods (e.g., for $N = 100$ and $\lambda = 0.3$, SEM RMSE improved from 1.400 with $R^2 = 0.1$ to 0.783 with $R^2 = 0.4$). Under misspecifications in the exogenous part of the model (Table 18), SEM again showed a higher RMSE (2.555) compared to all SAM methods at low reliability and small sample size. gSAM and SAM-ML performed better (RMSEs of 1.069 and 1.126, respectively), while SAM-ULS had a higher RMSE (1.541). RMSE improved with higher $R^2$ for both SEM and SAM methods (e.g., SEM RMSE decreased from 2.555 to 1.212 with an increase in $R^2$ from 0.1 to 0.4). For misspecifications in the endogenous factors (Table 19), SEM showed a larger RMSE (1.283) at low reliability and small sample size, compared to SAM methods. gSAM and SAM-ML had lower RMSEs (0.910 and 1.021, respectively), while SAM-ULS had a higher RMSE (1.074). The pattern of improvement with increasing $R^2$ held (e.g., SEM RMSE decreased from 1.283 to 0.766). Finally, for misspecifications in both exogenous and endogenous factors, SEM had the highest RMSE (1.861) at low reliability and small sample size, while gSAM and SAM-ML performed better (RMSEs of 0.972 and 1.059, respectively) and SAM-ULS had a higher RMSE (1.158). Again, RMSE improved with higher $R^2$ (e.g., SEM RMSE improved from 1.861 to 1.158).

```{r}
# Function to create a RMSE table for a specific model type with SEM performance included
create_rmse_table <- function(table_number, custom_caption, model_type_value) {
  summary_2 %>%
    filter(model_type == model_type_value) %>%
    filter(b == 3) %>%  # Filter for Measurement Blocks = 3
    select(N, reliability, R_squared, method, MeanRelativeRMSE) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `R²` = R_squared,
      `Method` = method,
      `Mean Relative RMSE` = MeanRelativeRMSE
    ) %>%
    mutate(
      `Mean Relative RMSE` = round(`Mean Relative RMSE`, 3),
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Mean Relative RMSE`) %>%
    kbl(caption = custom_caption,
    format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the tables for each model type with custom captions
rmse_table_correct <- create_rmse_table(1, "Mean Relative RMSE for vanilla SEM and different SAM methods under correct model specification (Model 2.1)", "2.1")
#rmse_table_correct

rmse_table_exo <- create_rmse_table(2, "Mean Relative RMSE for vanilla SEM and different SAM methods under Model Misspecifications (cross loadings and correlated residuals) in Exogenous Factors of the Model (Model 2.2-exo)", "2.2_exo")
#rmse_table_exo

rmse_table_endo <- create_rmse_table(3, "Mean Relative RMSE for vanilla SEM and different SAM methods under Model Misspecifications (cross loadings and correlated residuals) in Endogenous Factors of the Model (Model 2.2-endo)", "2.2_endo")
#rmse_table_endo

rmse_table_both <- create_rmse_table(4, "Mean Relative RMSE for vanilla SEM and different SAM methods under Model Misspecifications (cross loadings and correlated residuals) in Exogenous and Endogenous Factors of the Model (Model 2.2-both)", "2.2_both")
#rmse_table_both

```

### Improper Solutions

For small sample sizes (N = 100), SEM exhibits a high percentage of improper solutions, particularly at lower reliability levels (0.3). For example, with low regression weights and three measurement blocks, SEM has improper solution rates as high as 38.08% under the model with misspecifcations in the exogenous and endogenous part (table 24). In contrast, gSAM, lSAM-ML, and lSAM-ULS consistently show very low or zero improper solutions across all conditions. As sample size increases to 400 and 6400, the percentage of improper solutions in SEM decreases significantly, reaching nearly zero in most cases, while gSAM, lSAM-ML, and lSAM-ULS maintain their robustness with zero improper solutions across the board irrespective of misspecifcations. Lower reliability levels (0.3) and low regression weights exhibit the highest improper solution rates for SEM. For example, under model 2.1 with low regression weights, SEM shows improper solution rates of 21.45% for b = 3 and b = 5. Higher reliability levels (0.5 and 0.7) reduce the occurrence of improper solutions for SEM, aligning more closely with the other methods.Measurement block size (b = 3 or b = 5) has minimal impact on the percentage of improper solutions, indicating that the number of measurement blocks does not substantially affect the performance of the methods in terms of improper solutions. Model misspecifications impact SEM more significantly regarding improper solutions. SEM shows notably higher rates of improper solutions under all misspecification conditions, especially at smaller sample sizes and lower reliability levels. For example, under model 2.2_both with low reliability and low regression weights, SEM shows improper solution rates as high as 38.08%. In contrast, gSAM, lSAM-ML, and lSAM-ULS exhibit very low or zero improper solutions in the presence of model misspecifications, demonstrating their robustness and reliability compared to SEM.

```{r}
# Function to create an Improper Solutions table for a specific model type
create_improper_solutions_table <- function(table_number, custom_caption, model_type_value) {
  summary_2 %>%
    filter(model_type == model_type_value) %>%
    group_by(N, reliability, R_squared, b, method) %>%
    summarise(
      ImproperSolutionsCount = sum(ImproperSolutionsCount, na.rm = TRUE),
      .groups = 'drop'
    ) %>%
    mutate(
      ImproperSolutionsPercentage = round((ImproperSolutionsCount / 10000) * 100, 2),
      R_squared = recode(R_squared, `0.1` = "low", `0.4` = "high")
    ) %>%
    select(N, reliability, R_squared, b, method, ImproperSolutionsPercentage) %>%
    rename(
      `N` = N,
      `Reliability` = reliability,
      `R²` = R_squared,
      `Measurement Blocks` = b,
      `Method` = method,
      `Improper Solutions (%)` = ImproperSolutionsPercentage
    ) %>%
    mutate(
      Method = recode(Method,
                      SEM = "SEM",
                      gSAM = "GSAM",
                      lSAM_ML = "SAM-ML",
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Improper Solutions (%)`) %>%
    kbl(caption = custom_caption,
    format = "latex", booktabs = TRUE, escape = FALSE) %>%
    kable_styling(latex_options = c("striped", "hold_position"))
}

# Create and display the tables for each model type with custom captions
improper_solutions_table_1 <- create_improper_solutions_table(1, "Percentage of Improper Solutions for correct model specification (Model 2.1)", "2.1")
#improper_solutions_table_1

improper_solutions_table_2 <- create_improper_solutions_table(2, "Percentage of Improper Solutions for Model Misspecifications (cross loadings and correlated residuals) in Exogenous Factors of the Model (Model 2.2-exo)", "2.2_exo")
#improper_solutions_table_2

improper_solutions_table_3 <- create_improper_solutions_table(3, "Percentage of Improper Solutions for Model Misspecifications (cross loadings and correlated residuals) in Endogenous Factors of the Model (Model 2.2-endo)", "2.2_endo")
#improper_solutions_table_3

improper_solutions_table_4 <- create_improper_solutions_table(4, "Percentage of Improper Solutions for Model Misspecifications (cross loadings and correlated residuals) in Exogenous and Endogenous Factors of the Model (Model 2.2-both)", "2.2_both")
#improper_solutions_table_4
```

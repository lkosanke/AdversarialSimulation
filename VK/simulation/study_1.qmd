
---
title: "Study 1"
author: "Valentin Kriegmair"
date: '2024-04-21'
output: html_document
---

Remarks:
- This is yet in a combined markdown format but will be split into separate files.
- The study does not include all methods and performance metrics.
- At the moment this only runs locally.
- Once this basic structure of the script is confirmed the remaining methods and metrics will be added as well as the second study and the script will be set up to run on a cluster.

Questions:
- Is the general approach sound?
- Am I correctly using the `future` and `furrr` packages for parallel processing?
- Does this need major alterations to run on the cluster?

# Libraries
will be handled by docker or renv
```{r}
library(MASS)  
library(dplyr) 
library(tidyr) 
library(future) 
library(furrr)
library(lavaan)
library(purrr)
library(parallel)
```

# Setup
```{r}
RNGkind("L'Ecuyer-CMRG")
plan(multisession, workers = detectCores() - 1)
```

# Data Generating Mechanism:
```{r}
generate_dgm <- function(model_type, N, reliability) {
    n_factors <- 5
    n_indicators <- 15
    
    # Define matrices
    B <- matrix(0, nrow = n_factors, ncol = n_factors)
    Psi <- diag(c(1, 1, 0.97, 0.98, 0.98), n_factors)
    Lambda <- matrix(0, nrow = n_indicators, ncol = n_factors)
    
    # Set up Lambda matrix for factor loadings
    for (i in 1:n_factors) {
        Lambda[(i - 1) * 3 + (1:3), i] <- c(1, 0.7, 0.7)
    }
    
    # Set B matrix according to the model type
    if (model_type %in% c("1.1", "1.2", "1.4")) {
        B[3, c(1, 2, 4)] <- 0.1  # f3 influenced by f1, f2, and f4
        B[4, c(1, 2)] <- 0.1      # f4 influenced by f1 and f2
        B[5, c(3, 4)] <- 0.1      # f5 influenced by f3 and f4
    } else if (model_type == "1.3") {
        B[3, c(1, 2)] <- 0.1      # f3 influenced by f1 and f2
        B[4, c(1, 2, 3)] <- 0.1   # f4 influenced by f1, f2, and f3
        B[5, c(3, 4)] <- 0.1      # f5 influenced by f3 and f4
    }
    
    # Calculate Theta (unique variances) inversely proportional to reliability
    sigma_yy <- Lambda %*% Psi %*% t(Lambda)  # Variance explained by latent factors
    theta_diag <- diag(sigma_yy) * (1 - reliability)  # Unique variances based on reliability
    Theta <- diag(theta_diag)  # Start with a diagonal matrix

    # Adjust Theta for specific residual correlations in Model 1.3
    if (model_type == "1.3") {
        pairs <- cbind(c(2, 5, 8, 11, 14), c(3, 6, 9, 12, 15))
        for (pair in 1:ncol(pairs)) {
            i <- pairs[1, pair]
            j <- pairs[2, pair]
            Theta[i, j] <- Theta[j, i] <- 0.6 * min(theta_diag[c(i, j)])
        }
    }

    # Ensure all matrices are valid for Sigma calculation
    Sigma_eta <- solve(diag(1, n_factors, n_factors) - B) %*% Psi %*% t(solve(diag(1, n_factors, n_factors) - B))
    Sigma <- Lambda %*% Sigma_eta %*% t(Lambda) + Theta
    
    # Generate data
    data <- mvrnorm(n = N, mu = rep(0, n_indicators), Sigma = Sigma)
    colnames(data) <- paste0("y", 1:n_indicators)
    

    
        return(list(data = data, 
                true_values = list(B = B, Lambda = Lambda, Psi = Psi, Theta = Theta),
                B = B, Psi = Psi, Lambda = Lambda, Theta = Theta))
}
```

# Simulation Study 1
```{r eval=FALSE}
library(MASS)
library(dplyr)
library(tidyr)
library(future)
library(furrr)
library(lavaan)
library(purrr)

# Define the number of repetitions
n_reps <- 10 

# Parameter Grid
params <- expand.grid(
  model_type = c("1.1", "1.2", "1.3", "1.4"),
  N = c(100, 400, 6400),
  reliability = c(0.3, 0.5, 0.7),
  method = c("SEM", "SAM"),
  rep = 1:n_reps
) %>%
  mutate(id = row_number())
 
# Set population values
B_true <- c(
    'f3~f1' = 0.1, 'f3~f2' = 0.1, 'f3~f4' = 0.1,
    'f4~f1' = 0.1, 'f4~f2' = 0.1,
    'f5~f3' = 0.1, 'f5~f4' = 0.1
)
true_values <- list(
    B = B_true
)

run_study_1 <- function(params, true_values) {

  # Model syntax with constraints on variances to be non-negative
  model_syntax <- "
            f1 =~ y1 + y2 + y3
            f2 =~ y4 + y5 + y6
            f3 =~ y7 + y8 + y9
            f4 =~ y10 + y11 + y12
            f5 =~ y13 + y14 + y15

            f3 ~ f1 + f2 + f4
            f4 ~ f1 + f2
            f5 ~ f3 + f4

            # Constrain variances to be non-negative
            y1 ~~ vy1*y1
            y2 ~~ vy2*y2
            y3 ~~ vy3*y3
            y4 ~~ vy4*y4
            y5 ~~ vy5*y5
            y6 ~~ vy6*y6
            y7 ~~ vy7*y7
            y8 ~~ vy8*y8
            y9 ~~ vy9*y9
            y10 ~~ vy10*y10
            y11 ~~ vy11*y11
            y12 ~~ vy12*y12
            y13 ~~ vy13*y13
            y14 ~~ vy14*y14
            y15 ~~ vy15*y15
  
            vy1 > 0.01
            vy2 > 0.01
            vy3 > 0.01
            vy4 > 0.01
            vy5 > 0.01
            vy6 > 0.01
            vy7 > 0.01
            vy8 > 0.01
            vy9 > 0.01
            vy10 > 0.01
            vy11 > 0.01
            vy12 > 0.01
            vy13 > 0.01
            vy14 > 0.01
            vy15 > 0.01
  "

  # Wrap the run_analysis function with safely and quietly
  run_analysis <- function(data, model_syntax, method = "SEM") {
    if (method == "SEM") {
      fit <- sem(model_syntax, data = as.data.frame(data))
    } else if (method == "SAM") {
      fit <- sam(model_syntax, data = as.data.frame(data))
    } else {
      stop("Unknown method specified")
    }
    return(fit)
  }

  safe_quiet_run_analysis <- safely(quietly(run_analysis))

  # Evaluate method performance
  evaluate_performance <- function(fit, true_values) {
    if (is.null(fit) || !lavInspect(fit, "converged")) {
      return(list(Error = "Model did not converge or fit object is null"))
    }
    estimates <- coef(fit)
    estimated_paths <- estimates[names(estimates) %in% names(true_values$B)]
    calculate_metrics(estimated_paths, unlist(true_values$B))
  }

  # Performance Metrics calculation
  calculate_metrics <- function(estimated, true) {
    biases <- 100 * (estimated - true) / true  # Percentage bias
    rmse <- sqrt(mean((estimated - true)^2, na.rm = TRUE))
    list(Bias = mean(biases, na.rm = TRUE), RMSE = rmse)
  }

  # Run the simulations and analysis in parallel
  results <- future_pmap(params, function(model_type, N, reliability, method, rep, id) {
    set.seed(12345 + id)
    data <- generate_dgm(model_type, N, reliability)$data
    fit_result <- safe_quiet_run_analysis(data, model_syntax, method)

    if (!is.null(fit_result$result$result) && lavInspect(fit_result$result$result, "converged")) {
      performance_metrics <- evaluate_performance(fit_result$result$result, true_values)
      c(performance_metrics, Converged = 1, NonConverged = 0, 
        Warnings = toString(fit_result$result$warnings), 
        Messages = toString(fit_result$result$messages),
        Errors = if (is.null(fit_result$error)) NA_character_ else toString(fit_result$error$message))
    } else {
      c(Bias = NA_real_, RMSE = NA_real_, Converged = 0, NonConverged = 1, 
        Warnings = toString(fit_result$result$warnings), 
        Messages = toString(fit_result$result$messages),
        Errors = if (is.null(fit_result$error)) NA_character_ else toString(fit_result$error$message))
    }
  }, .options = furrr_options(seed = TRUE))

  # Create a dataframe for results
  results_df <- data.frame(
    ModelType = params$model_type,
    N = params$N,
    Reliability = params$reliability,
    Method = params$method,
    Bias = map_dbl(results, ~ as.numeric(.x["Bias"])),
    RMSE = map_dbl(results, ~ as.numeric(.x["RMSE"])),
    Converged = map_dbl(results, ~ as.numeric(.x["Converged"])),
    NonConverged = map_dbl(results, ~ as.numeric(.x["NonConverged"])),
    Warnings = map_chr(results, ~ as.character(.x["Warnings"])),
    Messages = map_chr(results, ~ as.character(.x["Messages"])),
    Errors = map_chr(results, ~ as.character(.x["Errors"]))
  )

  # Calculate summary statistics
  summary_stats <- results_df %>%
    group_by(ModelType, N, Reliability, Method) %>%
    summarise(
      AverageBias = mean(Bias, na.rm = TRUE),
      AverageRMSE = mean(RMSE, na.rm = TRUE),
      ConvergenceRate = mean(Converged),
      NonConvergenceCount = sum(NonConverged),
      .groups = 'drop'
    ) %>%
    arrange(ModelType, N, Reliability, Method)

  # Return the summary statistics, detailed results, and convergence rate
  list(Summary = summary_stats, DetailedResults = results_df)
}
```

# Results
```{r eval=FALSE}
# Run the complete simulation study
simulation_results <- run_study_1(params, true_values)
simulation_results$Summary
simulation_results$DetailedResults
```

# Setup
```{r}
# Define the number of repetitions
n_reps <- 10 

# Parameter Grid:
params <- expand.grid(
  model_type = c("1.1", "1.2", "1.3", "1.4"),
  N = c(100, 400, 6400),
  reliability = c(0.3, 0.5, 0.7),
  method = c("SEM", "SAM"),
  rep = 1:n_reps
) %>%
  mutate(id = row_number())

# Set population values:
B_true <- c(
    'f3~f1' = 0.1, 'f3~f2' = 0.1, 'f3~f4' = 0.1,
    'f4~f1' = 0.1, 'f4~f2' = 0.1,
    'f5~f3' = 0.1, 'f5~f4' = 0.1
)
true_values <- list(
    B = B_true
)
```

# Study 1 all metrics
```{r}
library(MASS)
library(dplyr)
library(tidyr)
library(future)
library(furrr)
library(lavaan)
library(purrr)
library(simhelpers)

# Define the number of repetitions
n_reps <- 10 

# Parameter Grid
params <- expand.grid(
  model_type = c("1.1", "1.2", "1.3", "1.4"),
  N = c(100, 400, 6400),
  reliability = c(0.3, 0.5, 0.7),
  method = c("SEM", "SAM"),
  rep = 1:n_reps
) %>%
  mutate(id = row_number())

# Set population values
B_true <- c(
  'f3~f1' = 0.1, 'f3~f2' = 0.1, 'f3~f4' = 0.1,
  'f4~f1' = 0.1, 'f4~f2' = 0.1,
  'f5~f3' = 0.1, 'f5~f4' = 0.1
)
true_values <- list(
  B = B_true
)

# Function to calculate coverage of confidence intervals
calculate_coverage <- function(fit, true_values) {
  if (is.null(fit) || !lavInspect(fit, "converged")) {
    return(NA)
  }
  # Extract parameter estimates with confidence intervals
  param_estimates <- parameterEstimates(fit)
  # Filter for the paths of interest
  param_estimates <- param_estimates %>%
    filter(lhs %in% c("f3", "f4", "f5") & op == "~")
  
  coverage <- sapply(names(true_values$B), function(param) {
    parts <- unlist(strsplit(param, "~"))
    row <- param_estimates %>%
      filter(lhs == parts[1], rhs == parts[2])
    if (nrow(row) > 0) {
      row$ci.lower <= true_values$B[param] && row$ci.upper >= true_values$B[param]
    } else {
      NA
    }
  })
  mean(coverage, na.rm = TRUE)
}

run_study_1 <- function(params, true_values) {

  # Model syntax with constraints on variances to be non-negative
  model_syntax <- "
            f1 =~ y1 + y2 + y3
            f2 =~ y4 + y5 + y6
            f3 =~ y7 + y8 + y9
            f4 =~ y10 + y11 + y12
            f5 =~ y13 + y14 + y15

            f3 ~ f1 + f2 + f4
            f4 ~ f1 + f2
            f5 ~ f3 + f4

            # Constrain variances to be non-negative
            y1 ~~ vy1*y1
            y2 ~~ vy2*y2
            y3 ~~ vy3*y3
            y4 ~~ vy4*y4
            y5 ~~ vy5*y5
            y6 ~~ vy6*y6
            y7 ~~ vy7*y7
            y8 ~~ vy8*y8
            y9 ~~ vy9*y9
            y10 ~~ vy10*y10
            y11 ~~ vy11*y11
            y12 ~~ vy12*y12
            y13 ~~ vy13*y13
            y14 ~~ vy14*y14
            y15 ~~ vy15*y15
  
            vy1 > 0.01
            vy2 > 0.01
            vy3 > 0.01
            vy4 > 0.01
            vy5 > 0.01
            vy6 > 0.01
            vy7 > 0.01
            vy8 > 0.01
            vy9 > 0.01
            vy10 > 0.01
            vy11 > 0.01
            vy12 > 0.01
            vy13 > 0.01
            vy14 > 0.01
            vy15 > 0.01
  "

  # Wrap the run_analysis function with safely and quietly
  run_analysis <- function(data, model_syntax, method = "SEM") {
    if (method == "SEM") {
      fit <- sem(model_syntax, data = as.data.frame(data))
    } else if (method == "SAM") {
      fit <- sam(model_syntax, data = as.data.frame(data))
    } else {
      stop("Unknown method specified")
    }
    return(fit)
  }

  safe_quiet_run_analysis <- safely(quietly(run_analysis))

  # Evaluate method performance
  evaluate_performance <- function(fit, true_values) {
    if (is.null(fit) || !lavInspect(fit, "converged")) {
      return(list(Error = "Model did not converge or fit object is null"))
    }
    estimates <- coef(fit)
    estimated_paths <- estimates[names(estimates) %in% names(true_values$B)]
    calculate_metrics(estimated_paths, unlist(true_values$B))
  }

  # Performance Metrics calculation
  calculate_metrics <- function(estimated, true) {
    data <- data.frame(estimates = estimated, true_param = true)
    calc_relative(data = data, estimates = data$estimates, true_param = data$true_param, criteria = c("relative bias", "relative rmse"))
  }
  
  # Run the simulations and analysis in parallel
  results <- future_pmap(params, function(model_type, N, reliability, method, rep, id) {
    set.seed(12345 + id)
    data <- generate_dgm(model_type, N, reliability)$data
    fit_result <- safe_quiet_run_analysis(data, model_syntax, method)

    if (!is.null(fit_result$result$result) && lavInspect(fit_result$result$result, "converged")) {
      performance_metrics <- evaluate_performance(fit_result$result$result, true_values)
      coverage <- calculate_coverage(fit_result$result$result, true_values)
      c(performance_metrics, Coverage = coverage, Converged = 1, NonConverged = 0, 
        Warnings = toString(fit_result$result$warnings), 
        Messages = toString(fit_result$result$messages),
        Errors = if (is.null(fit_result$error)) NA_character_ else toString(fit_result$error$message))
    } else {
      c(rel_bias = NA_real_, rel_bias_mcse = NA_real_, rel_rmse = NA_real_, rel_rmse_mcse = NA_real_, Coverage = NA_real_,
        Converged = 0, NonConverged = 1, 
        Warnings = toString(fit_result$result$warnings), 
        Messages = toString(fit_result$result$messages),
        Errors = if (is.null(fit_result$error)) NA_character_ else toString(fit_result$error$message))
    }
  }, .options = furrr_options(seed = TRUE))

  # Create a dataframe for results
  results_df <- data.frame(
    ModelType = params$model_type,
    N = params$N,
    Reliability = params$reliability,
    Method = params$method,
    RelativeBias = map_dbl(results, ~ as.numeric(.x["rel_bias"])),
    RelativeBiasMCSE = map_dbl(results, ~ as.numeric(.x["rel_bias_mcse"])),
    RelativeRMSE = map_dbl(results, ~ as.numeric(.x["rel_rmse"])),
    RelativeRMSEMCSE = map_dbl(results, ~ as.numeric(.x["rel_rmse_mcse"])),
    Coverage = map_dbl(results, ~ as.numeric(.x["Coverage"])),
    Converged = map_dbl(results, ~ as.numeric(.x["Converged"])),
    NonConverged = map_dbl(results, ~ as.numeric(.x["NonConverged"])),
    Warnings = map_chr(results, ~ as.character(.x["Warnings"])),
    Messages = map_chr(results, ~ as.character(.x["Messages"])),
    Errors = map_chr(results, ~ as.character(.x["Errors"]))
  )

  # Calculate summary statistics
  summary_stats <- results_df %>%
    group_by(ModelType, N, Reliability, Method) %>%
    summarise(
      AverageRelativeBias = mean(RelativeBias, na.rm = TRUE),
      AverageRelativeBiasMCSE = mean(RelativeBiasMCSE, na.rm = TRUE),
      AverageRelativeRMSE = mean(RelativeRMSE, na.rm = TRUE),
      AverageRelativeRMSEMCSE = mean(RelativeRMSEMCSE, na.rm = TRUE),
      AverageCoverage = mean(Coverage, na.rm = TRUE),
      ConvergenceRate = mean(Converged),
      NonConvergenceCount = sum(NonConverged),
      .groups = 'drop'
    ) %>%
    arrange(ModelType, N, Reliability, Method)

  # Return the summary statistics, detailed results, and convergence rate
  list(Summary = summary_stats, DetailedResults = results_df)
}

# Example usage (make sure to implement `generate_dgm`):
# results <- run_study_1(params, true_values)
```

# Results
```{r}
# Run the complete simulation study
simulation_results <- run_study_1(params, true_values)
simulation_results$Summary
simulation_results$DetailedResults
```
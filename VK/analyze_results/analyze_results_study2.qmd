---
title: "AnalyzeResults"
author: "Valentin Kriegmair"
execute: 
  echo: false
format:
  pdf:
    documentclass: article
    margin-left: 1in
    margin-right: 1in
    margin-top: 1in
    margin-bottom: 1in
  html:
    toc: true
bibliography: ../../bibliography.bib
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
library(dplyr)
library(tidyr)
library(kableExtra)
library(ggplot2)
library(data.table)
```

```{r}
summary_2 <- readRDS("../simulation/results/summary_study2.rds")
```

# Study 2

## Overview

This study expanded on the first by examining additional conditions to evaluate the performance of vanilla SEM compared to global SAM (gSAM), local SAM with maximum likelihood (lSAM-ML), and local SAM with unweighted least squares (lSAM-ULS). The following factors were modulated:

-   **Sample sizes**: small (N = 100), medium (N = 400), or large (N = 6400)
-   **Variance explained** ($R^2$) by the endogenous factors: low ($R^2 = 0.1$) or medium ($R^2 = 0.4$)
-   **Indicator reliability**: three indicators per factor, with conditions of all high (0.8), all low (0.5), and average low (0.5) varying between 0.7 to 0.3 with the highest reliability for the scaling indicator.
-   **Distribution**: normal or non-normal, with non-normal data generated by skewing factors and residuals with a kurtosis of -2 and 8, respectively.
-   **Structural misspecifications**: by varying the population model, omitting either:
    -   a residual covariance in the exogenous part of the model,
    -   a factor loading in the endogenous part of the model,
    -   a factor loading in the exogenous part of the model,
    -   a structural path in the endogenous part of the model,
    -   a structural path in the analysis model.
-   **Number of measurement blocks**: this refers to the number of separate measurement models fitted in the first step of SAM:
    -   Separate measurement model per latent variable (b = k = 5),
    -   Joint measurement model for all exogenous variables (b = 3).

In all conditions, the structural parameters were set with modulated b values according to the $R^2$ values, reflecting the explained variance of the endogenous factors by the exogenous factors. This approach aimed to assess the robustness and performance of the different SEM and SAM methodologies under varying levels of reliability, sample sizes, and model specifications.

## Convergence

Tables 1-4 below show convergence rates using each method - vanilla SEM, global SAM (gSAM), local SAM with maximum likelihood (lSAM-ML), and local SAM with unweighted least squares (lSAM-ULS) for different sample sizes, reliability, R-squared values, and measurement blocks under varying model (mis) - specifications.

For moderate and large sample sizes (N = 400, 6400), all methods achieve a 100% convergence rate across all conditions, indicating that sample size is a significant factor in ensuring model convergence.

For a small sample size (N = 100), SEM shows lower convergence rates, particularly at lower reliability levels (0.3), while GSAM, SAM-ML, and SAM-ULS maintain high convergence rates. This suggests that the reliability of indicators has a considerable impact on the convergence of SEM, with higher reliability levels (0.5 and 0.7) generally corresponding to higher convergence rates for all methods.

R-squared values (0.1 and 0.4) do not significantly impact the convergence rates across methods, as evidenced by consistent high convergence rates in all conditions except for SEM at the lowest reliability and sample size.

Measurement block size (b = 3 or b = 5) has minimal impact on convergence rates, indicating that the number of measurement blocks does not substantially affect the ability of the methods to converge.enarios with smaller sample sizes and lower reliability than vanilla SEM estimation.

Also under different model misspecifications SEM struggles more with convergence, particularly in scenarios with lower reliability and smaller sample sizes, while GSAM, SAM-ML, and SAM-ULS maintain high convergence rates across all misspecifications.

```{r}
# Function to create a convergence rate table for a specific model type
create_convergence_table <- function(model_type_label, model_type_value) {
  summary_2 %>%
    filter(model_type == model_type_value) %>%
    select(N, reliability, R_squared, b, method, ConvergenceRate) %>%
    rename(
      Reliability = reliability,
      R_Squared = R_squared,
      `Measurement Block (b)` = b,
      Method = method,
      `Convergence Rate` = ConvergenceRate
    ) %>%
    mutate(
      `Convergence Rate` = round(`Convergence Rate`, 3),
      Method = recode(Method, 
                      SEM = "SEM", 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Convergence Rate`) %>%
    kbl(caption = paste("Convergence Rates for (Mis-)Specification (Model):", model_type_label),
        format = "html", booktabs = TRUE) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
}

# Create and display the tables for each model type in Study 2
convergence_table_2_1 <- create_convergence_table("2.1", "2.1")
convergence_table_2_1

convergence_table_2_2_exo <- create_convergence_table("2.2_exo", "2.2_exo")
convergence_table_2_2_exo

convergence_table_2_2_endo <- create_convergence_table("2.2_endo", "2.2_endo")
convergence_table_2_2_endo

convergence_table_2_2_both <- create_convergence_table("2.2_both", "2.2_both")
convergence_table_2_2_both


```

## Average Relative Bias

Tables 5-8 below show the difference in mean relative bias between SEM and the other methods - global SAM (gSAM), local SAM with maximum likelihood (lSAM-ML), and local SAM with unweighted least squares (lSAM-ULS) for different sample sizes, reliability, R-squared values, and measurement blocks under varying model (mis) - specifications.

For all sample sizes (N = 100, 400, 6400), gSAM, lSAM-ML, and lSAM-ULS consistently show lower average relative bias compared to SEM, indicating that these methods generally provide more accurate parameter estimates.

Lower reliability levels (0.3) show the greatest improvement in bias reduction for all methods compared to SEM. As reliability increases to 0.5 and 0.7, the difference in bias between SEM and the other methods decreases, indicating that reliability significantly impacts bias, with lower reliability benefiting more from the alternative methods.

R-squared values (0.1 and 0.4) do not show a substantial impact on the relative bias differences across methods, suggesting that the proportion of variance explained by the model does not significantly affect the bias reduction capabilities of gSAM, lSAM-ML, and lSAM-ULS.

Measurement block size (b = 3 or b = 5) has minimal impact on the difference in bias, indicating that the number of measurement blocks does not substantially affect the bias reduction performance of the methods compared to SEM.

Model misspecifications impact SEM more significantly regarding bias. SEM shows notably higher relative biases under all misspecification conditions, especially at smaller sample sizes and lower reliability levels. For example, under model 2.2_exo, SEM shows a significantly higher bias when R-squared is 0.1 and reliability is 0.3, with differences as high as -0.709 in comparison to gSAM. Similarly, for model 2.2_both, SEM shows substantial biases in comparison to gSAM, lSAM-ML, and lSAM-ULS across various conditions. These results indicate that SEM is more sensitive to model misspecifications, leading to higher relative biases, whereas gSAM, lSAM-ML, and lSAM-ULS exhibit more robustness and lower bias in the presence of model misspecifications.

```{r}
# Function to create a relative bias table for a specific model type with differences to SEM
create_relative_bias_diff_table <- function(model_type_label, model_type_value) {
  sem_values <- summary_2 %>%
    filter(model_type == model_type_value, method == "SEM") %>%
    select(N, reliability, R_squared, b, MeanRelativeBias) %>%
    rename(
      Reliability = reliability,
      R_Squared = R_squared,
      `Measurement Block (b)` = b,
      SEM_Value = MeanRelativeBias
    )
  
  summary_2 %>%
    filter(model_type == model_type_value, method != "SEM") %>%
    select(N, reliability, R_squared, b, method, MeanRelativeBias) %>%
    left_join(sem_values, by = c("N", "reliability" = "Reliability", "R_squared" = "R_Squared", "b" = "Measurement Block (b)")) %>%
    mutate(Diff_to_SEM = MeanRelativeBias - SEM_Value) %>%
    select(N, reliability, R_squared, b, method, Diff_to_SEM) %>%
    rename(
      Reliability = reliability,
      R_Squared = R_squared,
      `Measurement Block (b)` = b,
      Method = method,
      `Difference to SEM` = Diff_to_SEM
    ) %>%
    mutate(
      `Difference to SEM` = round(`Difference to SEM`, 3),
      Method = recode(Method, 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Difference to SEM`) %>%
    kbl(caption = paste("Difference in Mean Relative Bias to SEM for (Mis-)Specification (Model):", model_type_label),
        format = "html", booktabs = TRUE) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
}

# Create and display the tables for each model type in Study 2
relative_bias_diff_table_2_1 <- create_relative_bias_diff_table("2.1", "2.1")
relative_bias_diff_table_2_1

relative_bias_diff_table_2_2_exo <- create_relative_bias_diff_table("2.2_exo", "2.2_exo")
relative_bias_diff_table_2_2_exo

relative_bias_diff_table_2_2_endo <- create_relative_bias_diff_table("2.2_endo", "2.2_endo")
relative_bias_diff_table_2_2_endo

relative_bias_diff_table_2_2_both <- create_relative_bias_diff_table("2.2_both", "2.2_both")
relative_bias_diff_table_2_2_both

```

```{r}
# # Function to create a coverage table for a specific model type with differences to SEM
# create_coverage_diff_table <- function(model_type_label, model_type_value) {
#   sem_values <- summary_2 %>%
#     filter(model_type == model_type_value, method == "SEM") %>%
#     select(N, reliability, R_squared, b, MeanCoverage) %>%
#     rename(
#       Reliability = reliability,
#       R_Squared = R_squared,
#       `Measurement Block (b)` = b,
#       SEM_Value = MeanCoverage
#     )
#   
#   summary_2 %>%
#     filter(model_type == model_type_value, method != "SEM") %>%
#     select(N, reliability, R_squared, b, method, MeanCoverage) %>%
#     left_join(sem_values, by = c("N", "reliability" = "Reliability", "R_squared" = "R_Squared", "b" = "Measurement Block (b)")) %>%
#     mutate(Diff_to_SEM = MeanCoverage - SEM_Value) %>%
#     select(N, reliability, R_squared, b, method, Diff_to_SEM) %>%
#     rename(
#       Reliability = reliability,
#       R_Squared = R_squared,
#       `Measurement Block (b)` = b,
#       Method = method,
#       `Difference to SEM` = Diff_to_SEM
#     ) %>%
#     mutate(
#       `Difference to SEM` = round(`Difference to SEM`, 3),
#       Method = recode(Method, 
#                       gSAM = "GSAM", 
#                       lSAM_ML = "SAM-ML", 
#                       lSAM_ULS = "SAM-ULS")
#     ) %>%
#     pivot_wider(names_from = Method, values_from = `Difference to SEM`) %>%
#     kbl(caption = paste("Difference in Mean Coverage to SEM for (Mis-)Specification (Model):", model_type_label),
#         format = "html", booktabs = TRUE) %>%
#     kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
# }
# 
# # Create and display the tables for each model type in Study 2
# coverage_diff_table_2_1 <- create_coverage_diff_table("2.1", "2.1")
# coverage_diff_table_2_1
# 
# coverage_diff_table_2_2_exo <- create_coverage_diff_table("2.2_exo", "2.2_exo")
# coverage_diff_table_2_2_exo
# 
# coverage_diff_table_2_2_endo <- create_coverage_diff_table("2.2_endo", "2.2_endo")
# coverage_diff_table_2_2_endo
# 
# coverage_diff_table_2_2_both <- create_coverage_diff_table("2.2_both", "2.2_both")
# coverage_diff_table_2_2_both


```

## RMSE

Tables 9-12 below show the difference in mean relative RMSE between SEM and the other methods - global SAM (gSAM), local SAM with maximum likelihood (lSAM-ML), and local SAM with unweighted least squares (lSAM-ULS) for different sample sizes, reliability, R-squared values, and measurement blocks under varying model (mis) - specifications.

For all sample sizes (N = 100, 400, 6400), gSAM, lSAM-ML, and lSAM-ULS consistently show lower RMSE compared to SEM, indicating that these methods generally provide more precise parameter estimates.

Lower reliability levels (0.3) show the greatest reduction in RMSE for all methods compared to SEM. As reliability increases to 0.5 and 0.7, the difference in RMSE between SEM and the other methods decreases, indicating that reliability significantly impacts RMSE, with lower reliability benefiting more from the alternative methods.

R-squared values (0.1 and 0.4) do not show a substantial impact on the relative RMSE differences across methods, suggesting that the proportion of variance explained by the model does not significantly affect the RMSE reduction capabilities of gSAM, lSAM-ML, and lSAM-ULS.

Measurement block size (b = 3 or b = 5) has minimal impact on the difference in RMSE, indicating that the number of measurement blocks does not substantially affect the RMSE reduction performance of the methods compared to SEM.

Model misspecifications impact SEM more significantly regarding RMSE. SEM shows notably higher RMSE under all misspecification conditions, especially at smaller sample sizes and lower reliability levels. For example, under model 2.2_exo, SEM shows a significantly higher RMSE when R-squared is 0.1 and reliability is 0.3, with differences as high as -1.486 in comparison to gSAM. Similarly, for model 2.2_both, SEM shows substantial RMSE in comparison to gSAM, lSAM-ML, and lSAM-ULS across various conditions. These results indicate that SEM is more sensitive to model misspecifications, leading to higher RMSE, whereas gSAM, lSAM-ML, and lSAM-ULS exhibit more robustness and lower RMSE in the presence of model misspecifications.

```{r}
# Function to create an RMSE table for a specific model type with differences to SEM
create_rmse_diff_table <- function(model_type_label, model_type_value) {
  sem_values <- summary_2 %>%
    filter(model_type == model_type_value, method == "SEM") %>%
    select(N, reliability, R_squared, b, MeanRelativeRMSE) %>%
    rename(
      Reliability = reliability,
      R_Squared = R_squared,
      `Measurement Block (b)` = b,
      SEM_Value = MeanRelativeRMSE
    )
  
  summary_2 %>%
    filter(model_type == model_type_value, method != "SEM") %>%
    select(N, reliability, R_squared, b, method, MeanRelativeRMSE) %>%
    left_join(sem_values, by = c("N", "reliability" = "Reliability", "R_squared" = "R_Squared", "b" = "Measurement Block (b)")) %>%
    mutate(Diff_to_SEM = MeanRelativeRMSE - SEM_Value) %>%
    select(N, reliability, R_squared, b, method, Diff_to_SEM) %>%
    rename(
      Reliability = reliability,
      R_Squared = R_squared,
      `Measurement Block (b)` = b,
      Method = method,
      `Difference to SEM` = Diff_to_SEM
    ) %>%
    mutate(
      `Difference to SEM` = round(`Difference to SEM`, 3),
      Method = recode(Method, 
                      gSAM = "GSAM", 
                      lSAM_ML = "SAM-ML", 
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Difference to SEM`) %>%
    kbl(caption = paste("Difference in Mean Relative RMSE to SEM for (Mis-)Specification (Model):", model_type_label),
        format = "html", booktabs = TRUE) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
}

# Create and display the tables for each model type in Study 2
rmse_diff_table_2_1 <- create_rmse_diff_table("2.1", "2.1")
rmse_diff_table_2_1

rmse_diff_table_2_2_exo <- create_rmse_diff_table("2.2_exo", "2.2_exo")
rmse_diff_table_2_2_exo

rmse_diff_table_2_2_endo <- create_rmse_diff_table("2.2_endo", "2.2_endo")
rmse_diff_table_2_2_endo

rmse_diff_table_2_2_both <- create_rmse_diff_table("2.2_both", "2.2_both")
rmse_diff_table_2_2_both


```

## Improper Solutions

Tables 13-16 below show the percentage of improper solutions using each method - vanilla SEM, global SAM (gSAM), local SAM with maximum likelihood (lSAM-ML), and local SAM with unweighted least squares (lSAM-ULS) for different sample sizes, reliability, regression weights, and measurement blocks under varying model (mis) - specifications.

For small sample sizes (N = 100), SEM exhibits a high percentage of improper solutions, particularly at lower reliability levels (0.3). For example, with low regression weights and three measurement blocks, SEM has improper solution rates as high as 33.15% under the 2.2_exo model. In contrast, gSAM, lSAM-ML, and lSAM-ULS consistently show very low or zero improper solutions across all conditions.

As sample size increases to 400 and 6400, the percentage of improper solutions in SEM decreases significantly, reaching nearly zero in most cases, while gSAM, lSAM-ML, and lSAM-ULS maintain their robustness with zero improper solutions across the board.

Lower reliability levels (0.3) and low regression weights exhibit the highest improper solution rates for SEM. For example, under model 2.1 with low regression weights, SEM shows improper solution rates of 21.45% for b = 3 and b = 5. Higher reliability levels (0.5 and 0.7) reduce the occurrence of improper solutions for SEM, aligning more closely with the other methods.

Measurement block size (b = 3 or b = 5) has minimal impact on the percentage of improper solutions, indicating that the number of measurement blocks does not substantially affect the performance of the methods in terms of improper solutions.

Model misspecifications impact SEM more significantly regarding improper solutions. SEM shows notably higher rates of improper solutions under all misspecification conditions, especially at smaller sample sizes and lower reliability levels. For example, under model 2.2_both with low reliability and low regression weights, SEM shows improper solution rates as high as 38.08%. In contrast, gSAM, lSAM-ML, and lSAM-ULS exhibit very low or zero improper solutions in the presence of model misspecifications, demonstrating their robustness and reliability compared to SEM.

```{r}
# Function to create an Improper Solutions table for a specific model type
create_improper_solutions_table <- function(model_type_label, model_type_value, summary_data) {
  summary_data %>%
    filter(model_type == model_type_value) %>%
    group_by(N, reliability, R_squared, b, method) %>%
    summarise(
      ImproperSolutionsCount = sum(ImproperSolutionsCount, na.rm = TRUE),
      .groups = 'drop'
    ) %>%
    mutate(
      ImproperSolutionsPercentage = round((ImproperSolutionsCount / 10000) * 100, 2),
      R_squared = recode(R_squared, `0.1` = "low", `0.4` = "high")
    ) %>%
    select(N, reliability, R_squared, b, method, ImproperSolutionsPercentage) %>%
    rename(
      Reliability = reliability,
      `Regression Weights` = R_squared,
      B = b,
      Method = method,
      `Improper Solutions (%)` = ImproperSolutionsPercentage
    ) %>%
    mutate(
      Method = recode(Method,
                      SEM = "SEM",
                      gSAM = "GSAM",
                      lSAM_ML = "SAM-ML",
                      lSAM_ULS = "SAM-ULS")
    ) %>%
    pivot_wider(names_from = Method, values_from = `Improper Solutions (%)`) %>%
    kbl(caption = paste("Percentage of Improper Solutions for (Mis-)Specification (Model):", model_type_label),
        format = "html", booktabs = TRUE) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
}

# Create and display the tables for each model type in Study 2
improper_solutions_table_2_1 <- create_improper_solutions_table("2.1", "2.1", summary_2)
improper_solutions_table_2_1

improper_solutions_table_2_2 <- create_improper_solutions_table("2.2_exo", "2.2_exo", summary_2)
improper_solutions_table_2_2

improper_solutions_table_2_3 <- create_improper_solutions_table("2.2_endo", "2.2_endo", summary_2)
improper_solutions_table_2_3

improper_solutions_table_2_4 <- create_improper_solutions_table("2.2_both", "2.2_both", summary_2)
improper_solutions_table_2_4
```

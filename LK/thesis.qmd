---
title: "Adversarial Simulation"
subtitle: "A case study"
author: "Leonard Kosanke"
format:
    pdf:
        fontsize: 12pt
        linestretch: 1.5
        geometry: "left=25mm, right=20mm, top=20mm, bottom=20mm"
        classoption: oneside
        papersize: a4
        header-includes:
           - \usepackage{float}
           - \floatplacement{table}{H}
fontsize: 12pt
bibliography: ../bibliography.bib
engine: knitr
---

The git hash is: `{r} suppressWarnings(system2("git", c("rev-parse", "--short=5", "HEAD"), stdout = TRUE, stderr = TRUE))`

```{r include = FALSE}
library(here)
library(knitr)
library(kableExtra)
library(tidyverse)
```

# Introduction

Monte Carlo simulations are an extensively utilized tool for assessing and comparing statistical methods in quantitative empirical science. 
They are used to evaluate the performance of estimation and inference methods by analyzing them in light of known simulated population models and values.
Despite the clear insight into the underlying data structures that simulations provide, they are not immune to common pitfalls that thus far have been predominantly associated with the replication crisis in empirical research [@lohmann_its_2022].
In simulation studies, a key challenge is ensuring that results can be generalized to real-world applications.
These studies must ensure that the chosen performance metrics, experimental factors, and inference models accurately reflect and test real-world scenarios. 
Given the impracticality of simulating every possible model, method and use case, these studies inherently involve a multitude of decision-making elements potentially prone to bias.
For example, the decisions of selecting evaluation criteria for competing methods (for example absolute versus relative bias of performance), selecting which models or methods to compare in the first place, and which simulation specifications to run (for example deciding on which sample size to simulate), can have substantive impact on the results.
Evidently, many issues of open science in the empirical realm are present in the context of simulation studies as well. 
One approach that addresses these issues at the level of value selection suggests to sample all relevant values from existing results published in the empirical literature of interest [@bollmann_what_2015]. 
An example application could be to decide on a simulated sample size not just based on a hunch or prior experience, but based on previous empirical studies conducted in the field. 
Even though this idea is fruitful and improves upon the problem of generalizability, the argument with regards to the arbitrariness of decision-making still remains. 
For example, deciding on which papers, or even which models within a paper to select (and which not), can still be subject to individual bias or simply chance. 
Additionally, conducting extensive literature reviews for all parameter values chosen is very time-consuming and difficult to apply in practice.

Another approach to tackle these challenges that has been proposed in the empirical context is Adversarial Collaboration (AC). 
Adversarial collaboration, piloted by @mellers_frequency_2001 and popularized by @kahneman_thinking_2011, is a research method increasingly recognized within the quantitative empirical research community for enhancing scientific rigor. 
Praised as “The next science reform” adversarial collaboration is the process of disagreeing scholars working jointly to resolve scientific disputes [@clark_adversarial_2021]. 
An ongoing effort by @melloni_adversarial_2023, for example, conducts Adversarial Collaboration between proponents of two different theories on the relationship between consciousness and brain activity, hoping to advance research in this field. 
Adversarial Collaboration entails identifying points of empirical disagreement, designing mutually agreed upon studies to test competing hypotheses, and jointly publishing results, irrespective of the outcome. 
The idea is that in conducting adversarial collaboration, fair comparison and truthful representation of opposing views can be achieved, thereby enhancing epistemic accountability and reducing research ambiguity in scientific decision-making. 
Additionally, juxtaposing and debating competing positions in this way can improve generalizability of results.

Our goal is to transfer the benefits of adversarial collaboration to simulation studies. 
To do so, we conducted a focused case study, utilizing an exemplary topic in the literature of simulation studies: comparing a newly proposed iterative structural after measurement (SAM) estimation approach to structural equation model (SEM) estimation with traditional, non-iterative SEM estimation [@dhaene_evaluation_2023; @robitzsch_comparing_2022; @rosseel_structural_2022]. 
In their respective simulations, the authors' results differed up to the point of contradiction, providing us with ideal grounds for conducting adversarial collaboration. 
While @dhaene_evaluation_2023 concluded that SAM estimation generally outperforms non-iterative SEM in small samples, @robitzsch_comparing_2022 did not find the methods to differ. 
Similarly, whereas the former found SAM to be more robust against model misspecification, the latter argued the opposite to be true. 
Applying Adversarial Collaboration, we, the authors of this paper, each represented one of these competing positions, as detailed later.

This allows us to examine the practical feasibility of adversarial collaboration in Monte Carlo simulation studies as well its potential to enhance methodological rigor and generalizability in this domain of research. 
Thus, this investigation leads us to address the following research question:

Can adversarial collaboration be applied to simulation studies, in terms of practical applicability and technical feasibility?

# Methods 

This section contains three parts. 
First, an outline of the AC framework we agreed upon before the start of the collaboration.
Then, each sides describes the methods they used to answer the substantive research question of the individual studies.

## Outline of the Adversarial Collaboration framework

To answer our research question, we created and followed the following procedure for the adversarial collaboration process: 
The procedure consists of two parts. In the first part, each researcher conducts an independent preliminary study as a replication of their side of the argument. 
In our case, Leonard Kosanke replicated relevant parts of the study by @robitzsch_comparing_2022, and Valentin Kriegmair replicated the study by @dhaene_evaluation_2023. 
The replications should include the generation of individual simulation protocols, as suggested by @morris_using_2019. 
In the second part, a joint study, including a joint simulation protocol, should be pursued. 
Here, the main part of the adversarial collaboration takes place. 
Each step of the individual simulation studies (from the substantive research question up until the interpretation of the results) is scrutinized and debated between collaborators, using adversarial collaboration techniques.
Decisions are made and documented based on the most convincing argument presented, if possible. 
This process starts based on the results of Part 1.
To facilitate a structured comparison and integration of studies through the collaboration process, we agreed on a framework for conducting our individual simulation studies in advance, that is visible in Table 1.  

Table 1: Structure of simulation studies 
1. Defining aims and objectives / Research Question of Interest. ( = verbal description) 
o as specific as possible 
o e.g. examination of goodness-of-fit statistics under varying degrees of 
misspecification 
o comparison of the maximum likelihood (ML) to (2SLS) 
2. Specification of Population Model 
o Optional and depending on field of simulation 
Modularities: 
o Structure: (e.g. CFA or SEM) 
o Size: number of latents and indicators 
o Complexity: (cross-loaded indicators, Reciprocal paths, Exogenous 
predictors) 
→ select target model for (assumed) general model types 
3. Data Generation Mechanism 
o resampling vs. parametric model draw 
o random number draw for data generation 
4. Experimental Design Simulation Procedures 
o Determine what factors to vary, on what levels, and whether fully, or partly 
factorially or one at a time (factor = scenario in Morris et al., 2019) 
o e.g. 
▪ sample size 
▪ distribution of the observed variables 
▪ extent of misspecification 
… 
5. Method Selection 
o Varies depending on research question 
o e.g. 
▪ type of and number of estimation methods to be compared 
6. Defining Estimands / Population level values 
o should reflect values commonly encountered in applied research. 
o e.g. 
▪ R2 values the chosen coefficients produce should also be reasonable 
for applied research. 
▪ parameters of the model should be statistically significant, even at the 
smallest sample size of the simulation. 
▪ consider power issues (e.g.:enough power to detect misspecification, 
too much power to detect misspecification at all sample sizes?) 
▪ “bias” in the estimates that will be introduced by the misspecification. 
7. Performance Measures 
o Selection and justification of use of e.g. 
▪ Bias 
▪ sensitivity/ specificity 
▪ predictive accuracy 
o decision on number of simulations for acceptable Monte Carlo SE for these 
measures 
8. Software Selection 
o to run simulation 
o packages & functions 
9. Analysis and Interpretation plan 
o Analysis: descriptive vs. inferential 
o Interpretation: decision criteria that evaluate performance : e.g. if 1-ß > 90, 
the method performs well 

Before Coding and Execution: Anticipating all critical decision processes (e.g. exclusion 
criteria for “imperfect” samples) 
10. Coding and Execution 
o Amount and content of scripts 
o include (sanity) checks 
o setting seeds 
o troubleshooting & verification 
11. Analyzing results 
o descriptive 
o graphical 
o inferential 
o exploration 
12. Reporting & Presentation 
o provide rationales for each choice made in the previous steps 
o publishing code and simulated data 

End of Table 1 

This framework allows for the expected divergences in results of the individual simulations, but at the same time provides a basis for systematic comparison and synthesis for the joint simulation. 
In our case study, we have identified two substantive research questions to co-align the individual simulation studies: 

1. How do SAM and traditional SEM methods (including ML and ULS) compare in terms of bias, Mean Squared Error (MSE), and convergence rates in small to moderate samples? 

2. What is the impact of model misspecifications, such as residual correlations and cross-loadings, on the performance of SAM compared to traditional SEM methods? 

In the end, each collaborator reports the results in their own paper.
If desired, a joint paper can be published in addition.

### Adversarial collaboration techniques

We are aware that adversarial collaboration has its limitations when it comes to decision making in joint studies, and has lead to unresolvable disagreements in previous studies [@cowan_how_2020; @mellers_frequency_2001). 
In order to mitigate this risk, next to each collaborator publishing their own paper, we propose to conduct the joint study in a structured and formalized manner. 
To this end, we identified several collaboration techniques presented in Table 2. 
These were also meant to facilitate the evaluation of the adversarial collaboration with regards to our main research question. 
 

Table 2: Adversarial Collaboration (AC) Techniques: 

At each step of decision making in the Joint Study, any of the following techniques 


Thesis of: Leonard Kosanke 

can be used, depending on their applicability: 

 

• Core Disagreements: Arrive at clearly defined core disagreements that might be the origin 
for conflicts. (Clark et al., 2022) 

 

• Assumption check: List, question, and categorize assumptions (Kardos Dexter, 2017) 

 

• Red-Teaming: Generating what if scenarios, to identify limitations of the adversaries’ 
approach (Kardos Dexter, 2017) 

 

• Quality of information (Kardos Dexter, 2017): checking the adversaries’ quality of 
evidence based on literature. 

 

• Third neutral arbiter: In case of fundamentally unresolvable disagreements, a third 
neutral arbiter (Aaron Peikert) will be consulted to try to resolve them. 

 

End of table 2 


### Documentation - Decision log 

As the number of decisions made and their documentation is large, only the most important results should be presented in the respective papers. 
In addition, the appendix should contain a separate decision log with a detailed and complete documentation of all decisions made.
Here, the summarized results of the AC-techniques used for each step of the simulation study, as well as their consequence for the decision-making process should be detailed. 
Another aspect of structuring the collaboration within the decision log, lies in the way decision making is implemented in the joint study of our framework. 
In our mind, decision making can be based on four distinct grounds: Evidence-based, pragmatic reasons, arbitrary reasons or other reasons (e.g. personal values, political issues). 
Firstly, we deem a decision evidence based, if one can find a clear answer for a disagreement, based on empirical evidence or in the literature more general. 
Secondly, to be able to keep the scope of this project, decisions can be made for pragmatic reasons, as for example in the presence of time constraints. 
Thirdly, arbitrary reasons could be any agreement where the first too grounds are not present, but still a decision has to be made. 
Importantly, while this reason does not help in deciding for an option directly, it helps in understanding how often there is no substantive or pragmatic reason for decisions in simulation studies.
If this is the case, one might resort to deciding at random. 
Lastly, there might be other reasons that lead to a decision, that can not be anticipated but should still be captured.  
These four reasons ground decision-making and aim to ease the collaborative process by giving a structure. 
In some cases, more than one of these reasons can be present. 
In such cases, we their relevance to a decision could be ranked, if possible. An example is the log entry in figure 1: 


Figure 1: Exemplary log entry for deciding for a range of sample sizes 

 

Decision element: factor of sample sizes 

 

Result: c(50, 100, 200) 


 

Grounds: Primary - Evidence-based, as per Peikert et al. (2020); 

 

Secondary – pragmatic reason, based on the average of individual suggestions. 

 

Evaluation - Diary entries 

 

End of figure 1 

### Diary

The adversarial collaboration in Part 2 (Joint study) should be documented and evaluated from each collaborators subjective perspective using semi-structured diary entries based on @shah_exploring_2016, after each step.
This has the purpose of accumulating data for the evaluation of the AC and the collaboration procedure we propose. 
In our case, collecting this qualitative data was supposed to help us answer our research question. 
The evaluation of the diary entry can be conducted in a semi-structured and comparative manner, resorting for example to analysis of quantity of words to identify common themes of the collaboration. 

## Individual study by Kriegmair

## Individual study by Kosanke

The structure of this section closely aligns to our agreed upon structure of simulation studies.

In a first step, I published a simulation protocol containing all the planned analysis to be replicated from the original paper by @robitzsch_comparing_2022.
This protocol can be accessed here: <https://github.com/lkosanke/AdversarialSimulation/blob/main/LK/simulation_protocol.pdf>.

**Aims, objectives and research questions**

For my individual study, I replicated parts of @robitzsch_comparing_2022 that were relevant to our substantive research questions:

1. How do SAM and traditional SEM methods (including ML and ULS) compare in terms of bias, Mean Squared Error (MSE), and convergence rates in small to moderate samples? 

2. What is the impact of model misspecifications, such as residual correlations and cross-loadings, on the performance of SAM compared to traditional SEM methods? 

Overall, I conducted 6 simulation studies.

**Population Models and Data Generation Mechanisms**

The most important details with regards to the population models and data-generating mechanisms are visible in Table 1.

```{r schematic 1, eval = TRUE, echo = FALSE, out.width="110%"}

knitr::include_graphics(here::here("LK/images/TableOverviewIndividualStudies.pdf"))

```

With regards to the population models, all factors in all studies loaded onto 3 indicators each.
I chose the population values to align to the original paper by @robitzsch_comparing_2022.
For more details on the exact values of each study, see the simulation scripts in the github repository.
The multivariate normally distributed data was generated parametrically, based on a specified population model.
All simulations were conducted using seeds to allow for reproducibility.

**Experimental Design of simulation procedures**

Overall, 3 different types of factors were varied that can be deducted from Table 1 and are detailed again in the simulation scripts provided.
Firstly, we varied the Sample Size in all studies, ranging from N = 50 to 100.000.
I included a smaller sample size N=50 for all studies, to be able to answer our substantive research questions in more detail.
Study 1b explicitly investigated the small sample bias of LSAM estimation in low sample sizes. 
Thus, only N=50 and N=100 are present in this study.
Additionally, the amount of misspecification was varied in all studies, either via different numbers of unmodelled residual correlations, cross-loadings, or both.
In Studies 1b and 4a, some population values for model parameters (phi, beta and/ or lambda) were varied, as well.
Besides studies 1 and 2, full factorial designs were implemented.
In Studies 1 and 2 I omitted conditions were both one positive and one negative value would be present.
I hypothesize that this was done in @robitzsch_comparing_2022 to avoid cancellation of biases, but the authors do not give reasoning for this decision themselves.
Studies 4 and 4a looked at the differential performance of the estimators in a model that included a non-saturated structural model (i.e. regressions between some of the factors).
These studies were replications not only of the paper by @robitzsch_comparing_2022, but of the first paper on the SAM approach by @rosseel_structural_2022.
In contrast to the other studies, studies 4 and 4a differed in the way the misspecification variation was labelled in @robitzsch_comparing_2022.
Instead of varying a factor misspecification as in the previous study, they vary 3 different data-generating mechanisms (DGM's) as a whole.
Thus the conditions are labelled differently:
DGM 1 contained no misspecification. DGM 2 contained 5 cross-loadings in the data-generating model, that were not modelled in the estimated models. DGM 3 contained 20 residual correlations that were not modelled in the models.
I extended them to investigate the interaction of beta and N for the 5-factor regression model, as this again was of interest four our substantial research questions.
Additionally, I omitted the inclusion of DGM 1 in Study 4a, as it neither contained misspecification (which is central to our research question), nor lead to interesting results in the original study.

**Method Selection**

In terms of estimation methods, I used constrained maximum likelihood and unweighted least squares estimation, so that loadings and variance parameters were given the constraints that they had to be positive and larger than 0.01.
These were implemented in  classical SEM-estimation, as well as both in their LSAM and GSAM variants.
Exceptions were studies 1b, 4 and 4a, where only LSAM was investigated, as results did not really differ between the two different SAM-methods.

**Performance Measures**

I calculated the bias and RMSE of the estimated factor correlations in all studies, as well as the standard deviation of the one factor correlation present in Studies 1,2 and 3.
For the type of bias calculated, I oriented on @robitzsch_comparing_2022, besides in Study 1b.
Thus, I calculated average relative bias in Studies 1, 2 and 3, and average absolute bias in Studies 1b, 4 and 4a.
In Study 1b, I took the absolute value to see if negative and positive biases canceled each other out in the original study for conditions with lower phi values.
In Addition to what was done in @robitzsch_comparing_2022, I calculated confidence intervals for the bias estimates, but omitted them in the results tables for presentation purposes.
The exact computation of the performance measures is detailed in the simulation scripts and results.pdf file in my sub-folder of the github repository.

I did not include a detailed mechanism to capture model convergence as detailed in the first substantive research question.
As @robitzsch_comparing_2022 argued in their paper, and was showed already in other simulations, using constrained maximum likelihood estimation should resolve convergence issues of classical maximum likelihood estimation in smaller samples.
I did include, however, a mechanism to track the total number of warnings for each estimation and compare it to the total number of estimations as a sanity check.

For interpretation of results, I oriented on cut-offs that were used in the original paper by @robitzsch_comparing_2022.
For Bias, I interpreted difference of 0.05 or higher as substantial.
For SD, I explicitly mentioned percentage reductions of more or equal to 5%.
For RMSE, the same interpretation was used for differences of 0.03 or higher. 
The simulation was repeated 1500 times for each Study.

**Software**

All analyses were conducted in R.
I used the packages lavaan, purrr, tidyverse, furrr to conduct the simulations, as well as knitr and kableExtra for presenting the results.

# Results

After agreeing on our two substantive research question, we started the AC by conducting our individual studies.
We replicated and in some parts extended on the results of @robitzsch_comparing_2022 and @dhaene_evaluation_2023.
Our simulation protocols can be accessed in our Github-repository, under releases: <https://github.com/lkosanke/AdversarialSimulation/tree/main/>.
The next sections presents the results of these studies and their integration.

## Individual study by Kriegmair


## Individual study by Kosanke

The full result analysis for my individual study is available here: <https://github.com/lkosanke/AdversarialSimulation/blob/main/LK/results.pdf>.
In this section, I will focus on the most important results only.
For the most part, results from @robitzsch_comparing_2022 have been successfully replicated:
I did not observe substantial convergence issues in any study.
Across studies, as in the original paper, SAM did not generally outperform SEM in small to moderate samples.
SAM exhibited a negative small sample bias that made SAM appear superior in conditions with unmodelled positive cross-loadings and residual correlations.
This bias was especially strong for lower lambda and higher phi or beta values.
Going ahead of what was investigated in @robitzsch_comparing_2022, I found that this bias is also present in models with lower phi or beta values.
Thus, it cannot be concluded that SAM is more robust in models with non-saturated structural parameters.
If there was no misspecification or unmodelled negative cross-loadings and residual correlations, SAM tended to perform worse than traditional SEM, as far as can be concluded from my results.

### Convergence

As @robitzsch_comparing_2022 argued in their paper, I did not expect convergence issues due to constrained ML estimation that only allows for positive variances and loadings.
Nevertheless, I captured all messages, warnings and errors that occurred during the simulations.
No messages and errors were present in any of the studies.
Multiple warnings were observed in the first 4 simulations, some of them referring to potential problems with convergence.
Overall, the number of these warnings was very small compared to the total number of estimations performed.
They amounted to between 0.5-1.8%.
In studies 4 and 4a, an even smaller number of warnings was present, amounting to problems in 0.02% of estimations in study 4 and 0.1% in study 4a.
These warnings referred to potential problems with positive definite matrices and model identification.
In total, these numbers are negligible in size and align with the report of @robitzsch_comparing_2022, that convergence issues were not substantial for my estimations.
Additionally, a larger number of warnings was present with regards to the computation of fit indices in these final two studies.
As we were not interest in fit indices in our research question, they are not relevance for us.
A detailed analysis of all the warnings was conducted in the results.pdf document in my sub-folder of the github repository.

### Conditions without misspecification

Mainly Studies 1 and 4 investigated the comparative performance of SAM vs. traditional SEM estimation under correctly specified models.
Here it became apparent, that in absence of misspecification, none of two estimation methods clearly outperformed the other.
In Study 4, only slight, but no substantial differences could be observed in terms of bias and RMSE between the LSAM- and classical ML-estimation. 
This is visible in Table 1 for the example of RMSE.
Here, LSAM-ML appeared to outperform both SEM-estimation methods only in N=50.
For higher N, no differences were present between any of the estimators.

```{r eval = TRUE, echo = FALSE}

# Load the absolute bias results
bias_ci_s4 <- readRDS("SimulationResultsProcessed/sim4_abs_bias_ci.rds")

#Change create_styled_table function names function for abs_bias
create_styled_table <- function(data, condition) {
  kbl(data, 
      col.names = c("Method/Metric", "50", "100", "250",
                    "500", "1000", "2500", "1e+05"), booktabs = TRUE) %>%
    kable_styling(full_width = F, position = "left",
                  latex_options ="scale_down") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Condition:" = 8), bold = TRUE, align = "l",
                     line = TRUE, italic = TRUE) %>%
    footnote(general = paste("Condition:", condition))
}

```

```{r eval = TRUE, echo = FALSE}
rmse_s4 <- readRDS("SimulationResults/sim4_rmse.rds")

create_styled_table(rmse_s4[["DGM_0"]], "DGM_0")

```


```{r eval = TRUE, echo = FALSE}

# Load the relative bias results
bias_ci_s1 <- readRDS("SimulationResultsProcessed/sim1_rel_bias_ci.rds")

shorten_names <- function(df) {
  df$method_metric <- sub("_rel_bias", "", df$method_metric)
  return(df)
}

# Function to create a styled table for each condition
create_styled_table <- function(data, condition) {
  data <- shorten_names(data)
  data <- data %>%
    mutate(across(where(is.numeric), ~ round(., 3)))
  
  # Format table
  kbl(data,
      col.names = c("Method", "50", "100", "250", "500", 
                    "1000", "2500", "1e+05"), booktabs = TRUE) %>%
    kable_styling(full_width = F, position = "left",
                  latex_options =c("scale_down", "hold_position")) %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Condition:" = 8), bold = TRUE,  align = "l", 
                     line = TRUE, italic = TRUE) %>%
    footnote(general = paste("Condition:", condition))
}

```

In Study 1, both traditional SEM approaches outperformed all SAM estimators in small to moderate samples of N=50-500. 
This is true for both relative bias and RMSE, and visible for the former in Table 2.
Here, SAM's small sample bias is already visible as well.

```{r eval = TRUE, echo = FALSE}
create_styled_table(bias_ci_s1[["0_0.12"]], "0_0.12")
```


### Conditions with negatively valenced unmodelled parameters

Studies 1 and 2 explicitly investigated negatively valenced unmodelled parameters in the generating model.
In these studies, it became apparent that traditional SEM outperformed SAM estimation.

As can be seen in Table 1, both SEM estimators outperformed all four SAM estimators in terms of relative bias with two negative residual correlations present.

```{r eval = TRUE, echo = FALSE}
create_styled_table(bias_ci_s1[["2_-0.12"]], "2_-0.12")

```

The same was true in Study 2, in the presence of two negative cross-loadings.
In both these cases, bias values overall remained high but substantially less so in the traditional SEM methods. when comparing them in small to moderate sample sizes.

Importantly, no differences between the two approaches arose in these two examples in terms of RMSE, as can be seen in Table X for the negative cross-loadings in study 2.

```{r eval = TRUE, echo = FALSE}

rmse_s2 <- readRDS("SimulationResults/sim2_rmse.rds")
create_styled_table(rmse_s2[["2_-0.3"]], "2_-0.3")
```

### Conditions with positively valenced unmodelled parameters

In terms of performance for positively valenced cross-loadings and residual correlations, SAM appeared to slightly outperform traditional SEM estimation, but not in all scenarios. 

Table 3 shows this finding in Study 3, in conditions with both one unmodelled residual correlation and cross-loading.
Only from N=100-1000 did SAM outperform SEM.

```{r eval = TRUE, echo = FALSE}
# Load the relative bias results
bias_ci_s3 <- readRDS("SimulationResultsProcessed/sim3_rel_bias_ci.rds")

# Round all numeric values to 3 digits
bias_ci_s3 <- bias_ci_s3 %>%
  mutate(across(where(is.numeric), ~ round(., 3)))

#Format table for results
kbl(bias_ci_s3,
    col.names = c("Method/Metric", "50", "100", "250", 
                  "500", "1000", "2500", "1e+05"), booktabs = TRUE) %>%
    kable_styling(full_width = F, position = "left",
                  latex_options ="scale_down") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Study 3:" = 8), bold = TRUE,
                     align = "l", line = TRUE, italic = TRUE)

```

```{r eval = TRUE, echo = FALSE}

# Load the absolute bias results
bias_ci_s4 <- readRDS("SimulationResultsProcessed/sim4_abs_bias_ci.rds")

#Change create_styled_table function names function for abs_bias
create_styled_table <- function(data, condition) {
  kbl(data, 
      col.names = c("Method/Metric", "50", "100", "250",
                    "500", "1000", "2500", "1e+05"), booktabs = TRUE) %>%
    kable_styling(full_width = F, position = "left",
                  latex_options ="scale_down") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Condition:" = 8), bold = TRUE, align = "l",
                     line = TRUE, italic = TRUE) %>%
    footnote(general = paste("Condition:", condition))
}

```

In Study 4, a comparative advantage of LSAM compared to SEM-ML was present, but only for smaller samples.

With regards to RMSE, results were mixed as well. 
LSAM appeared to outperform in Table X for DGM 2 of Study 4.
In other conditions, however, no substantial differences arose in terms of RMSE.
```{r}

create_styled_table(rmse_s4[["DGM_1"]], "DGM_1")

```


### Small sample bias in LSAM estimation

The small sample Bias of LSAM estimation in Study 1b revealed that in smaller samples, ranging from N=50 to N=100, both LSAM-ML and LSAM-ULS estimation were biased.

Table 5 showed this to be especially apparent in a sample size of 50.
```{r eval = TRUE, echo = FALSE}
#Load processed results
bias_ci_s1b <- readRDS("SimulationResultsProcessed/sim1b_abs_bias_ci.rds")
```

```{r eval = TRUE, echo = FALSE}
#Formatted tables
kbl(bias_ci_s1b[["N_50"]][["LSAM_ML"]], 
    col.names = c("Lambda", "phi=0", "phi=0.2", 
                  "phi=0.4", "phi=0.6", "phi=0.8"), booktabs = TRUE) %>%
  kable_styling(full_width = F, position = "left", 
                latex_options ="scale_down") %>%
  add_header_above(c(" " = 1, "Phi values" = 5)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 1b: Absolute Bias of LSAM-ML for N=50" = 6),
                   bold = TRUE, align = "l", line = TRUE, italic = TRUE)

```

Note that absolute values of bias were calculated in my paper.
Consequently, the values of the bias should be interpreted as negative, as follows from the results of the original paper [@robitzsch_comparing_2022].
The bias persisted, but to a lesser degree in samples of 100.
Overall, comparing LSAM-ML and -ULS estimation, results were very similar.
Importantly, differential effects due to lambda and phi were present in Study 1b.
The small sample bias was especially strong for lower lambda and higher phi values, thus in contexts of low reliability and high factor correlations.
Also, a new insight is that the bias remained relevant for low values of phi, unlike in the original paper by @robitzsch_comparing_2022.
In consequence, there seemed to be no conditions were SAM's small sample bias was negligible.

As an additional investigation of the small sample bias, I included Study 4a to see its effect come to play in a 5-factor-model with regressions.
Table X shows the performance of SEM-ML, whereas Table Y shows the performance of LSAM-ML in DGM 2.

```{r eval = TRUE, echo = FALSE}
bias_ci_s4a <- readRDS("SimulationResultsProcessed/sim4a_abs_bias_ci.rds")

kbl(bias_ci_s4a[["DGM_1"]][["SEM_ML_metrics"]], 
    col.names = c("beta", "N=50", "N=100", "N=250", "N=500", 
                  "N=1000", "N=2500", "N=100.000"), booktabs = TRUE) %>%
  kable_styling(full_width = F, position = "left",
                latex_options ="scale_down") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: Absolute bias of SEM-ML for DGM 1" = 8),
                   bold = TRUE, align = "l", line = TRUE, italic = TRUE)

kbl(bias_ci_s4a[["DGM_1"]][["LSAM_ML_metrics"]], 
    col.names = c("beta", "N=50", "N=100", "N=250", "N=500",
                  "N=1000", "N=2500", "N=100.000"), booktabs = TRUE) %>%
  kable_styling(full_width = F, position = "left",
                latex_options ="scale_down") %>%
  add_header_above(c(" " = 1, "Sample size" = 7)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 4a: Absolute bias of LSAM-ML for DGM 1" = 8),
                   bold = TRUE, align = "l", line = TRUE, italic = TRUE)
```

Aligning with the findings of Study 1b, the results suggested an even better relative performance of LSAM- over traditional SEM-ML estimation for smaller N and higher beta.
Thus, we can see the negative small sample bias come into play in this study as well.
Results looked very similar with regards to RMSE.
Note that this trend was less strong, but still present in the conditions of DGM 3 when looking at residual correlations.




### Conclusions from both studies

Comparing the result from both individual studies, it became evident that they are at no point contradictory or in disagreement.
This is because of some central distinctions between the studies.
Firstly, same as in @robitzsch_comparing_2022, the use of constrained ML estimation solves both the convergence issues in small samples, as well as the positive small sample bias of the estimator.
The evident performance differences observed between traditional SEM and SAM estimation in the study by Kriegmair are thus to be taken cautiously.

Additionally, the results with regards to negatively valenced unmodelled residual correlations and cross-loading paint a more complete picture of the comparative performance of the two estimation methods.
The apparent performance advantages for traditional SEM in these conditions balance the advantages of SAM in positive conditions.
In the context of empirical studies, the valence of potential residual correlations and cross-loadings is not known a priori.
If that would be the case, one could simply turn to an estimator that is biased in the appropriate direction, to compensate this misspecification.
Thus, achieving unbiased estimates is desirable in application contexts.
The results of the individual studies leave us with no imminent call for additional investigation in the joint study.
What we and others viewed as a disagreement up to the point of contradiction, did not turn out to be one, upon closer examination.
In consequence, we did not conduct a joint study and stopped the collaboration at this point.

## Results of the collaboration

Going back to the outline of our collaboration framework, we successfully agreed on two research questions, conducted individual studies and integrated their results.
In consequence, we did not find remaining disagreement or additional scenarios of interest that would warrant to conduct the second part of the joint study.
This means that we were unable to completely test our framework.
Still, we have gained some insights with regards to our research question of whether adversarial collaboration can be applied to simulation studies, in terms of practical applicability and technical feasibility.

### Practical applicability

Already the first step of arriving at common research questions is a collaborative effort that requires to change perspective, and more deeply understand what the adversaries did in their studies.
We see the main success of the research question formulation in the focus on small to moderate sample sizes.
This allowed us to focus on specific parts of our replications and lead me, for example, to include an additional sample size in all my studies, as well as omit studies 5 and 6 that were conducted on the population level in the paper by @robitzsch_comparing_2022.
When conducting the individual studies, we knew that we would have would have to be completely transparent with our results and had to give explanations for our decisions, as they would be scrutinized by the adversary later on.
This increased quality of reporting, rigor and transparency in our individual studies.
After the individual studies, we integrated our results and looked for disagreement and points for further investigation.
We achieved a fair and truthful comparison of views and could agree that they were not opposing each other.
We also believe that we increased generalizability simply by integrating the findings of two studies into one conclusion for more data generating scenarios and conditions than one individual study conducted.

With regards to the practical applicability of the collaboration of the second part, we want to emphasize that the reason for not conducting the second part was not that it was not practically applicable, but that there was no substantial reason to do it.
We observed already the possibility to converge on research questions and found our individual simulations to be generally very compatible.
We believe this to be evidence, that the second part of the collaboration should be practically applicable.
Still, at the end of the day we could not conduct the second part of our collaboration framework.
We believe that here, a lot of potential lies to further promote our intended goals and investigate our research question.

### Technical feasibility.

Technical feasibility in the collaboration was achieved for the parts we could implement in our case study.
Utilizing tools like github with its functionalities to review code and jointly work on the same project allowed us to conduct simulations collaboratively in terms of comparing results, giving feedback and having access to the adversaries individual study within the same github repository.
We see a lot of potential for these tools to be used in the second part of our collaboration framework as well, to be able to split work within one study, give feedback on each others implementations via pull-requests, and much more.
Another aspect of technical feasibility was the access to a computing cluster, that we had.
This allowed a higher limit on computational complexity, and thus more freedom to include additional conditions to test, without fearing to much runtime or having to limit the number of repetitions.
Again, as we could not conduct the second part of our collaboration framework, the technical feasibility of the collaborative study can not be judged conclusively.

# Discussion

In this focused case study, we aimed to conduct adversarial collaboration to investigate the question if adversarial collaboration can be applied to simulation studies, in terms of practical applicability and technical feasibility.
To do so, we created and tested a collaboration framework that start with each adversary conducting an individual study, before collaborating in a joint study.
For this, we agreed on two substantial research questions to judge the comparative performance of traditional SEM vs. SAM estimation in small to moderate samples and under misspecifications.
After finishing our individual studies, the supposed disagreement turned out not to be one, as results aligned well.
Thus, we did not conduct a joint study and were only partly able to answer our research question.

For the first part of our framework, we found some evidence for practical applicability and technical feasibility:
We were able to agree on two focussed research question, that nudged our individual studies to focus on points of potential disagreement.
Additionally, we were both able to successfully plan and conduct our individual simulations in a way that allowed integration of our results and to converge on a shared understanding of their consequences. 
Thus, we achieved a fair and truthful representation of both sides views.
Technical feasibility was present with the use of github for sharing and accessing our own and the adversaries results. Additionally, usage of a computing cluster allowed us to freely include conditions where we deemed it necessary, in our individual studies.

For the second part of our framework, it remains unclear whether collaboration would be applicable and technically feasible in simulation studies.
Another attempt should be pursued to be able to answer our research question.

Even though we did not get to the point of direct collaboration in the second part of our framework, we are confident that it should be practically applicable and technically feasible:
Facilitating advancement in discussion between adversaries with the structured use of the tools we presented, we are confident that the necessary convergence for a joint study is achievable.
This should be even more so the case, because of some key differences between empirical and simulation studies:
Firstly, opposing opinions in simulation studies are less incomensurable in terms of testing.
In empirical studies, opposing theories might have completely different paradigms, measures and even construct definitions to answer the same research question.
In simulation studies, these differences are much smaller.
There is much more consensus on what data generating scenarios of interest could be, how performance is measured, and what the opposing methods are.
Another difference is the luxury of simulation studies to have much fewer resource restrictions.
The main resources are time and computation power, which usually are not too limited.
This allows adversaries to not have to find compromise in all cases, and, if in doubt, just include all conditions that might be relevant.
In empirical studies, there are much more limited resources such as number of participants, money, labs and much more.
Thus, aspects of power have to always be considered and this leads to having to have much more compromise in AC.

In terms of technical feasibility, we believe that using tools like github with its functionalities to review code and jointly work on the same project, should allow to conduct simulations collaboratively.
Ideally, additional resources like a computing cluster should be present to guarantee a higher limit on computational load, and thus more freedom to include additional conditions to test.
This, however, should not be a problem for research conducted in universities or institutes that usually have access to computing clusters.
Still, besides these assumptions and learnings from our first case study, we were unable to actually test the collaboration and can not say that the practical applicability and technical feasibility is given.

With regards to limitations there are several aspects to consider.
First and foremost, we are aware that we tested a very superficial "simulation" of adversariality.
Neither side had long scientific experience in our substantial field of interest, and thus the stakes were much lower than as if the original authors would have collaborated.
In that case, collaboration most likely would have been more difficult.
Still, including the individual studies as a first step, we think we did a good job of priming ourselves to the views of our respective sides of the argument.

Another important aspect to discuss is whether we should have been able to see the lack of disagreement earlier, even before conducting our individual studies.
As most of our findings were replications of the original papers, we might have been able to see their alignment if our preparation of the substantive topic would have been more in depth.
To avoid this, a more explicit and detailed evaluation of all the papers of interest could be done before deciding on adversarial collaboration, to make sure that there is actually conflicting evidence present.

Another question is in how far the advantages we observed with regards to increased rigor, transparency and open access were incremental benefits of adversarial collaboration, over and above the usage of simulation protocols and our open access approach to science.
We believe this not to be problematic, as AC and other aspects of open science can complement each very nicely and reinforce positive effects.
Thus, researchers who are not generally interested in open science, but in collaboration, would still be nudged to work more openly, which we view as a positive outcome.

In the end, we believe that it would be worthwile for future research to conduct another case study like ours.
Science is, in the end, a collaborative effort.
Exploring the limits of collaboration is thus valuable to investigate opportunities to advance scientific progress.



\newpage
# Bibliography
---
title: "Adversarial Simulation"
subtitle: "A case study"
author: "Leonard Kosanke"
format:
    pdf:
        fontsize: 11pt
        linestretch: 1.2
        geometry: "left=15mm, right=15mm, top=30mm, bottom=30mm"
        classoption: twoside
        papersize: a4
fontsize: 11pt
bibliography: ../bibliography.bib
engine: knitr
---

The git hash is: `{r} suppressWarnings(system2("git", c("rev-parse", "--short=5", "HEAD"), stdout = TRUE, stderr = TRUE))`

# Deviations from the simulation protocol

## Across studies

For pragmatic purposes of the simulation, we were unable to track and include non-convergence explicitly and exclude cases where it occured.

## Studies 1,2,3

For all these studies, 3 deviations are present.
Firstly, the total number of conditions turned out to be higher, as the calculation in the protocol forgot the addition of N=50 to be added to the factor sample size.
Secondly, It was not detailed what type of LSAM estimation will be implemented. For completeness sake, we included ML- as well as ULS-LSAM estimation.
Lastly, instead of just calculating the relative bias as originally planned, we also calculated the absolute bias to mitigate biases of opposite directions canceling each other out, as rightly pointed out in study 4 of @robitzsch_comparing_2022.

## Study 1b

In Study 1b, contrary to the simulation protocol, we added another sample size with N=50 to get a more nuanced understanding of the small-sample bias of LSAM. 
For the same reasons as in studies 1,2 and 3, we used ML as well as ULS LSAM estimation. Calculation of the relative Bias was not feasible in this study, as for the conditions where phi is 0, dividing by 0 would lead to infinite values.

## Study 4

Again, I forgot the addition of N=50 in the original condition calculation. 
Importantly, the model parameters and resulting population-level covariance matrices for the different DGM's provided by @robitzsch_comparing_2022 in their github repository were not the same as in the first paper by @rosseel_structural_2022: They omitted, for example, the cross-loading of the last factor in DGM2, without an apparent reason. Additionally, they had different values for the residual variances without giving insight into why they chose to do so and how (there was no calculation in the simulation script provided). For these reasons, and because of the fact that the calculation of the values in @rosseel_structural_2022 was replicable, we chose to use the values of @rosseel_structural_2022 by generating them with the same functions they used. In consequence, a fourth cross-loading was present in DGM 2.
The only difference in our data-simulation is that we only used the 3 out of 4 misspecification conditions that were present in @robitzsch_comparing_2022. 

## Study 4a

I decided to include a variation of study 4a after all, as the relation between beta/phi values and N is interesting: 
The original paper showed that results differed between estimators, depending on the size of the factor correlation in Studies 5, 6 and 1b, as well as differential effects due to N in Study 1b.
We deemed it interesting to investigate these results in a model with a different number of factors.
These findings, combined with our goal to investigate realistic data conditions, rather than just the population level, left us to include Study 4a, with the addition of an additional factor sample size.
As DGM1 neither contained misspecification (which is central to our research question), nor did it lead to interesting results in the original study 4, it was omitted.
Similarly, as no substantial conclusions were drawn with regards to RMSE in the original papers study 1 and 2, we omitted its calculation as well.
Lastly, for the same reasons as argued for Study 4, we again created the DGM's based on the functions used in @rosseel_structural_2022.

## Studies 5,6

As we are not interested in the comparative performance of SAM vs. SEM at the population level for our research question, but in the performance comparison under realistic conditions, especially in small to moderate sample sizes, we decided to not replicate Studies 5 and 6 after all.
We deemed this a fair deviation from our simulation protocol, especially because no substantial performance differences arose in the results of these studies, and the most relevant aspect for us, the differential effect due to the size of factor correlation, is already covered in two different models in studies 1b and 4a.

# Result analysis

## Original paper

### Bias
In the original paper, @robitzsch_comparing_2022 had some inconsistencies when it came to the interpretation of the results of their simulation studies. 
They did not thoroughly define an a prior criterium for when an estimator outperformed another. 
Only in their Studies 5 and 6, they defined an absolute bias equal to or larger than 0.05 as relevant. 
Additionally, when an estimator had a minimum percentage error of 20% higher than another, they defined it as being better than the other, in these studies.
Across the other studies, interpretations sometimes varied. 
In some studies (e.g. Study 1), they seemed to orient on the same value of 0.05, but this time as an absolute difference value, without the addition of a certain percentage cut-off. 
In other studies (e.g. Study 2), they seemed to deviate from this value to define relevant difference in absolute bias.
Importantly, this deviation was present in both directions, and therefore did not appear to be consciously biased towards one direction of possible outcomes.
In consequence, it appears that interpretations aren't coherent across the studies.

Another important aspect to highlight in the original papers results refers to the general presence of a negative bias in LSAM estimation. 
Even though this bias is correctly identified and explained already in Study 1, @robitzsch_comparing_2022 conduct multiple follow-up studies only investigating positive values in the misspecifications, namely studies 3-6. 
This is done without a general note of some sorts, that this bias should apply in these data-generating scenarios the same as in Studies 1 and 2.
In consequence, the interpretations @robitzsch_comparing_2022 make, are somewhat misleading on first sight, if one does not account for this bias.
This does not, however, explain why they did not include more studies or conditions with negatively valenced residual correlations or cross-loadings. We hypothesize, that the goal of the paper was to show that even in these favorable constellations, SAM does not perform better than SEM.

To be fair, it should be stated that most of the final conclusions drawn in the paper are correct, anyway. 
With the use of the term "generally", @robitzsch_comparing_2022 are right in that there are many data constellations, were SAM does not outperform traditional SEM.
They also acknowledge, that for non-saturated structural models, SAM seems to be the better choice.

For our studies, in order to have a more coherent interpretation of results, we will define an estimator to outperform another, based on the cut-off of 0.05 for bias.

### SD and RMSE

Even though @robitzsch_comparing_2022 report tables with results for SD and RMSE, they rarely explicitly mentioned them in the results sections or give cut-offs for substantial relevance.

SD is only reported and mentioned in the conditions without misspecification in Study 1, even though also claimed to be calculated in studies 2 and 3. 
In this study, they mentioned a 6.6% reduction of SD for LSAM in comparison to ML. 
This calculation, however, is wrong as the reduction amounts to 8%.
For our interpretation, we will report percentage reductions larger than 5% as well and compare them with the original study.

RMSE (and average RMSE) is reported in Studies 1-4, but only in studies 2 and 4 explicitly interpreted for our estimators of interest.
In study 2, differences of up to 0.03 are present and interpreted as not being substantial differences.
In study 4, differences of 0.01 are present and interpreted as slight efficiency gains.
We infer that this lack of mentioning and consistency can be interpreted as there being no differences of interest. 
Thus, we expect our studies to yield similar results. 
If, however, there are differences large than 0.03, we will report them, without drawing a substantive conclusion.

## Packages
```{r}
library(knitr)
library(kableExtra)
library(tidyverse)
```

# Results Simulation 1: 2-factor-CFA with 0,1 or 2 correlated residuals

## Error, warnings and messages
First, lets check for any messages, warnings or errors:

```{r}
#| ouput: true

#Save all errors, warnings and messages
all_messages_s1 <- readRDS("SimulationResults/sim1_results_error.rds")

#errors
errors_s1 <- all_messages_s1$errors
errors_sum_s1 <- unlist(errors_s1)
length(errors_sum_s1)

#warnings
warnings_s1 <- all_messages_s1$warnings
warnings_sum_s1 <- unlist(warnings_s1)
length(warnings_sum_s1)

#messages
messages_s1 <- all_messages_s1$messages
messages_sum_s1 <- unlist(messages_s1)
length(messages_sum_s1)

```

No errors and messages were present. There were, however, 245 warnings, we will investigate in detail.

### Warnings investigation

Investigating the 'warnings' object list, it became apparent that all the warnings are the same: "no non-missing arguments to min; returning Inf".
```{r}
#| ouput: true

unique(warnings_s1)

```
 
This warning could indicate either convergence issues or improper solutions due to small sample size in one or multiple estimators.
Unfortunately, due to the implementation of the study, we are unable to check for more details, as whether there is any kind of pattern with regards to the warning occurrence. 
There is, however, evidence that this occurs frequently standard SEM estimation and was one claimed advantage of SAM- over SEM-estimation [@rosseel_structural_2022].
Even though in @robitzsch_comparing_2022 the authors claim that constraining estimation to realistic values (e.g. loadings between 0 and 1) should solve this, applying these same constraints still lead to these issues in our study.
The combined study should check this more properly to evaluate these claims thoroughly.
As the 245 are less than 0.5% of the 525000 estimations performed, we will continue with the interpretation as planned, as their impact is negligable.

### Print all tables

```{r}
#| ouput: true

# Load the relative bias results
bias_ci_s1 <- readRDS("SimulationResults/sim1_rel_bias_ci.rds")

# Display the relative bias results for each condition, without formatting
#for (condition in names(bias_ci)) {
#  cat("## Condition:", condition, "\n\n")
#  print(bias_ci[[condition]])
#  cat("\n\n")
#}

# Function to create a styled table for each condition
create_styled_table <- function(data, condition) {
  kbl(data, col.names = c("Method/Metric", "50", "100", "250", "500", "1000", "2500", "1e+05"), format = "html") %>%
    kable_styling(full_width = F, position = "left") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Condition:" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE) %>%
    footnote(general = paste("Condition:", condition))
}

# Loop through each condition and print the styled table
for (condition in names(bias_ci_s1)) {
  print(create_styled_table(bias_ci_s1[[condition]], condition))
  cat("\n\n")  # Add space between tables
}


```

## Bias

General interpretation: Biased if Bias estimate larger than 0.05

Interpretation layers within one condition:

- Unbiased judgement SAM vs. SEM

- ALL across increasing N

- All within one N

-----------------


### Print all tables

```{r}
#| ouput: true

# Load the relative bias results
bias_ci_s1 <- readRDS("SimulationResults/sim1_rel_bias_ci.rds")

# Display the relative bias results for each condition, without formatting
#for (condition in names(bias_ci)) {
#  cat("## Condition:", condition, "\n\n")
#  print(bias_ci[[condition]])
#  cat("\n\n")
#}

# Function to create a styled table for each condition
create_styled_table <- function(data, condition) {
  kbl(data, col.names = c("Method/Metric", "50", "100", "250", "500", "1000", "2500", "1e+05"), format = "html") %>%
    kable_styling(full_width = F, position = "left") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Condition:" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE) %>%
    footnote(general = paste("Condition:", condition))
}

# Loop through each condition and print the styled table
for (condition in names(bias_ci_s1)) {
  print(create_styled_table(bias_ci_s1[[condition]], condition))
  cat("\n\n")  # Add space between tables
}


```

### Condition without correlated residuals (0_0.12)

```{r}
#| ouput: true

print(create_styled_table(bias_ci_s1[["0_0.12"]], "0_0.12"))

```

Both standard SEM estimators are unbiased (i.e. relative Bias < 0.05) across all N conditions.
The 4 SAM estimators are negatively biased for sample sizes up to 500 and have nearly identical estimation results.
Across all estimators, the relative Bias decreases with increasing N. 

Thus, SEM estimation outperforms SAM in small to moderate sample sizes, when there is no missspecification.

### Condition with one negative residual correlation (1_-0.12)

```{r}
#| ouput: true

print(create_styled_table(bias_ci_s1[["1_-0.12"]], "1_-0.12"))

```

All estimators are negatively biased (i.e. relative Bias >= -0.05) across all N conditions.
Across all estimators except SEM-ULS, the Bias decreases substantially with increasing N.
For SEM-ULS, it increases slightly but unsubstantially up to N=500, but is lowest overall.
The 4 SAM estimators have nearly identical estimation results.
In relative comparison, the two SEM estimators outperform all SAM estimators in every condition of N, even though this difference is not substantive for N=1000 and higher.

Thus, SEM estimation outperforms SAM in small to moderate sample sizes, when there is one negative residual.

### Condition with one positive residual correlation (1_0.12)

```{r}
#| ouput: true

print(create_styled_table(bias_ci_s1[["1_0.12"]], "1_0.12"))

```

Besides SEM-ML in N=50, both SEM estimators are positively biased across all N, with no visible trend for increasing N.
All SAM estimators have nearly identical values and increasing relative bias for increasing N.
They are negatively biased up to N=250 and positively biased for the other N conditions. 
For N=250 and N=500, their relative bias is unsubstantial.
In relative comparison, the two SEM estimators outperform all SAM estimators for sample sizes up to 100. For all higher N, the SAM estimators outperform the SEM estimators, even though the difference is unsubstantial starting from N=1000.

Thus, SEM estimation outperforms SAM in smaller samples (N=50-100), whereas SAM outperforms SEM in moderate samples (N250-500), and slightly in higher sample sizes.

### Condition with two negative residual correlations (2_-0.12)

```{r}
#| ouput: true

print(create_styled_table(bias_ci_s1[["2_-0.12"]], "2_-0.12"))

```

All estimators are negatively biased across all N conditions.
Across all estimators except SEM-ULS, the Bias decreases substantially with increasing N.
For SEM-ULS, it slightly, but unsubstantially increases up to N=500, but is lowest overall in these conditions.
The 4 SAM estimators have nearly identical estimation results.
In relative comparison, the two SEM estimators outperform all SAM estimators in every condition of N, even though this difference is not substantive for N=1000 and higher.

Thus, SEM estimation outperforms SAM in small to moderate sample sizes, when there are two negative residuals.

### Condition with two positive residual correlation (2_0.12)

```{r}
#| ouput: true

print(create_styled_table(bias_ci_s1[["2_0.12"]], "2_0.12"))

```

Both SEM estimators are positively biased across all N, with no visible trend for increasing N for ULS, but a substantial increase for ML.
All SAM estimators have nearly identical values and increasing relative bias for increasing N.
They are negatively biased up to N=100 and positively biased for the other N conditions. 
In relative comparison, the two SEM estimators outperform all SAM estimators for N=50. For all higher N, the SAM estimators outperform the SEM estimators (substantially, besides ML in N=100), even though the difference is unsubstantial starting from N=1000.

Thus, SEM estimation outperforms SAM in small samples (N=50), whereas SAM outperforms SEM in small to moderate samples (N100-500), and slightly in higher sample sizes.

### Summary

in 0.5% of estimations, a warning referring to potential problems with convergence or improper solutions due to small sample sizes in one or multiple estimators occured. 
This should be investigated more explicitly in the joint study.

In conditions with no, one or two negative residual correlations,  SEM outperforms SAM in small to moderate sample sizes. This seems to be independent of the amount of misspecification (1 vs. 2).

Comparing the conditions with one and two positive residual correlations, the results suggest that SAM outperforms SEM in smaller to moderate samples, with increasing amount of misspecifiation (even better for 2 than for 1 residual correlation). In small samples, SEM tends to perform better.

### Comparison with original paper

Comparing these results with the original paper, the results are  similar and can be seen as mostly replicated. The only difference lies in the potential convergence issues in a small number of estimations. 
Besides that, we draw the same conclusions as @robitzsch_comparing_2022:
All SAM approaches appear to be generally negatively biased for small to moderate samples in the 2-factor-CFA model. 
This leads to the comparatively worse performance in the conditions with no, one or two negative residual correlations. Additionally, this negative bias corrects for positive misspecifications and makes SAM appear superior in these conditions.


## SD

General interpretation: Substantial if percentage reductions are larger than 5%

Interpretation layers within one condition:

- Unbiased judgement SAM vs. SEM

- ALL across increasing N

- All within one N

-----------------

### print all tables
```{r}
#| ouput: true

# Load the relative bias results
sd_s1 <- readRDS("SimulationResults/sim1_sd.rds")


# Loop through each condition and print the styled table
for (condition in names(sd_s1)) {
  print(create_styled_table(sd_s1[[condition]], condition))
  cat("\n\n")  # Add space between tables
}
```

### No residual correlations (0_0.12)

```{r}
#| ouput: true

print(create_styled_table(sd_s1[["0_0.12"]], "0_0.12"))

```

Across all estimators, the standard deviation decreases with increasing N. 

There are only two substantial differences:
Substantial percentage reduction of SD up to 22.4% when comparing SEM-ML and LSAM-ML in N=50: 
0.299*x = 0.232
0.232/0.299 = 0.775
1-(0.232/0.299) = *0.224*

Substantial percentage reduction of SD up to 8.6% when comparing SEM-ML and LSAM-ML in N=100:
1-(0.19/0.208)

Thus, SAM outperforms SEM in terms of SD reduction in smaller samples

### One negative residual correlation (1_-0.12)
```{r}
#| ouput: true

print(create_styled_table(sd_s1[["1_-0.12"]], "1_-0.12"))

```

Across all estimators, the standard deviation decreases with increasing N. 

There are only two substantial differences:
Substantial percentage reduction of SD up to 27.4% when comparing SEM-ML and LSAM-ML in N=50: 
1-(0.233/ 0.321)

Substantial percentage reduction of SD up to 12.2% when comparing SEM-ML and LSAM-ML in N=100:
1-(0.187/0.213)

Thus, SAM outperforms SEM in terms of SD reduction in smaller samples.

### One positive residual correlation (1_0.12)
```{r}
#| ouput: true

print(create_styled_table(sd_s1[["1_0.12"]], "1_0.12"))

```

Across all estimators, the standard deviation decreases with increasing N. 

The SAM estimators outperform both SEM estimators in N=50 with a percentage reduction of SD up to 16.9%.
ULS-SEM outperforms all SAM estimators with a percentage reduction of SD up to 8.16% in N=100-250.
No substantial differences between the two SEM and all SAM estimators starting from moderate sample sizes (N=500-2500). 

Thus, SAM outperforms SEM in terms of SD reduction in small samples, whereas SEM outperforms SAM in moderate samples.

### Two negative residual correlations (2_-0.12)
```{r}
#| ouput: true

print(create_styled_table(sd_s1[["2_-0.12"]], "2_-0.12"))

```

Across all estimators, the standard deviation decreases with increasing N. 

The SAM estimators outperform both SEM estimators in N=50 with a percentage reduction of SD up to 29% and in N=100 with up to 12.8%.
For higher N, there are no substantial differences.

Thus, SAM outperforms SEM in terms of SD reduction in smaller samples.

### Two positive residual correlations (2_0.12)
```{r}
#| ouput: true

print(create_styled_table(sd_s1[["2_0.12"]], "2_0.12"))

```

Across all estimators, the standard deviation decreases with increasing N. 

The SAM estimators outperform both SEM estimators in N=50 with a percentage reduction of SD up to 15.2%.
Both SEM estimators ourperform the SAM estimators in N=100 with a percentage reduction of SD up to 11.1% and in N=250 with up to 10.3%.
For higher N, there are no substantial differences.

Thus, SAM outperforms SEM in terms of SD reduction in small samples, but SEM outperforms SAM in smaller to moderate samples.

### Summary and comparison with original paper

Overall, SAM outperforms SEM in terms of SD reduction in small to smaller samples, especially for negatively correlated residuals. 
In smaller to moderate samples, SEM outperforms SAM only for positively correlated residuals.
For conditions with no misspecification, which were the only conditions reported in @robitzsch_comparing_2022, the percentage reduction favoring SAM is nearly identical for N=100, and the reduction seems to be even more significantly in favor of SEM in smaller samples.

## RMSE
differences large than 0.03, we will report, without drawing a substantive conclusion.

Interpretation layers within one condition:

- ALL across increasing N

- All within one N

-----------------

###Print all tables
```{r}
rmse_s1 <- readRDS("SimulationResults/sim1_rmse.rds")

```

### No residual correlations (0_0.12)

```{r}
#| ouput: true

print(create_styled_table(rmse_s1[["0_0.12"]], "0_0.12"))

```

Across all estimators, the RMSE decreases with increasing N.
In N=100, both SEM estimators were more efficient than all  SAM estimators.
Besides, there are no substantial differences present.

### One negative residual correlation (1_-0.12)

```{r}
#| ouput: true

print(create_styled_table(rmse_s1[["1_-0.12"]], "1_-0.12"))

```

Across all estimators, the RMSE decreases with increasing N.
For small to moderate samples (N=50-250), both SEM estimators were more efficient than all SAM estimators.
For higher N, there are no substantial differences present.

### One positive residual correlation (1_0.12)

```{r}
#| ouput: true

print(create_styled_table(rmse_s1[["1_0.12"]], "1_0.12"))

```

Across all estimators, the RMSE decreases with increasing N.
For smaller samples (N=50-100), SEM-ULS was more efficient than all SAM estimators.
For higher N, there are no substantial differences present.

### Two negative residual correlations (2_-0.12)

```{r}
#| ouput: true

print(create_styled_table(rmse_s1[["2_-0.12"]], "2_-0.12"))

```

Across all estimators, the RMSE decreases with increasing N.
For small to moderate samples (N=50-500), The two SEM estimators were more efficient than all SAM estimators.
For higher N, there are no substantial differences present.

### Two positive residual correlations (2_0.12)

```{r}
#| ouput: true

print(create_styled_table(rmse_s1[["2_0.12"]], "2_0.12"))

```

Across all estimators, the RMSE decreases with increasing N.
For moderate samples (N=250-500), all SAM estimators were more efficient than the two SEM estimators.
For higher and lower N, there are no substantial differences present.

### Summary

Overall, classic SEM estimation was more efficient than SAM estimation in small to moderate samples. Only in the case of two positive residual correlation, SAM was more efficient than SEM in moderate samples.
It would be interesting to investigate higher amount of misspecification to investigate differential effects (Study 4?).

## Summary Study 1

We gained the following insights with regards to the 2-factor-CFA model:
Unlike the original paper by @robitzsch_comparing_2022, we experienced some issues that are related to convergence or improper solutions in small samples.

In terms of bias in small to moderate samples, SEM outperforms SAM in conditions with no, one and two negative residual correlations. SAM appears to outperform SEM for positive residual correlations. This, however, is an artifact of its negative small sample bias and not its performance. The results align with the ones obtained by the original paper.

With regards to SD and RMSE, results are incoherent with no clear pattern emerging. 

Thus, it cannot be stated that one estimation method is generally performing better than the other, based on the results of Study 1.

# Results Simulation 1b: 2-factor-CFA with 2 residual correlations and varying lambda and phi

## Error, warnings and messages
First, lets check for any messages, warnings or errors:

```{r}
#| ouput: true

#Save all errors, warnings and messages
all_messages_s1b <- readRDS("SimulationResults/sim1b_results_error.rds")

#errors
errors_s1b <- all_messages_s1b$errors
errors_sum_s1b <- unlist(errors_s1b)
length(errors_sum_s1b)

#warnings
warnings_s1b <- all_messages_s1b$warnings
warnings_sum_s1b <- unlist(warnings_s1b)
length(warnings_sum_s1b)

#messages
messages_s1b <- all_messages_s1b$messages
messages_sum_s1b <- unlist(messages_s1b)
length(messages_sum_s1b)

```

No errors and messages were present. There were, however, 414 warnings, we will investigate in detail.

### Warnings investigation

Investigating the 'warnings' object list, it became apparent that all the warnings are the same: "no non-missing arguments to min; returning Inf".

```{r}
unique(warnings_s1b)
```

As before, this warning could indicate either convergence issues or improper solutions due to small sample size in one or multiple estimators.
Again, we are unable to check for more details, but hypothesize that this occurs mainly in the standard SEM estimators.
There is, however, evidence that this occurs frequently standard SEM estimation and was one claimed advantage of SAM- over SEM-estimation [@rosseel_structural_2022].
The combined study should check this more properly to evaluate these claims thoroughly.
As the 414 estimation are around 0.5% of the 75000 estimations performed, we will continue with the interpretation as planned, as their impact is negligable.

## Bias

General interpretation: Biased if Bias estimate larger than 0.05

Interpretation layers within table condition:

- Unbiased judgement overall

- ALL across increasing lambda

- ALL across increasing phi

- All within one lambda 
- All within one phi

Layers between tables:

-Comparison between estimators

-Comparison between N's

-----------------


### Conditions with N=50
```{r}
#| ouput: true

# Load the relative bias results
bias_ci_s1b <- readRDS("SimulationResults/sim1b_abs_bias_ci.rds")

#Normal print
#print(bias_ci_s1b[["N_50"]][["LSAM_ML"]])
#print(bias_ci_s1b[["N_50"]][["LSAM_ULS"]])

#Formatted tables
kbl(bias_ci_s1b[["N_50"]][["LSAM_ML"]], col.names = c("Lambda", "phi=0", "phi=0.2", "phi=0.4", "phi=0.6", "phi=0.8"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Phi values" = 5)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 1b: Absolute Bias of LSAM-ML for N=50" = 6), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

kbl(bias_ci_s1b[["N_50"]][["LSAM_ULS"]], col.names = c("Lambda", "phi=0", "phi=0.2", "phi=0.4", "phi=0.6", "phi=0.8"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Phi values" = 5)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 1b: Absolute Bias of LSAM-ULS for N=50" = 6), bold = TRUE, align = "left", line = TRUE, italic = TRUE)


```

Comparing LSAM-ML and LSAM-ULS, the values are nearly identical. 
Therefore, the interpretation is the same for both estimators.

Across all conditions, both estimatos were biased.

For higher values of lambda, the estimators had less absolute Bias. This difference increased for higher phi values.

For lower values of lambda, the absolute bias increased with increasing phi. This trend reversed for higher values of lambda, where the absolute bias decreased with higher phi values.

### Conditions with N=100
```{r}
#| ouput: true

kbl(bias_ci_s1b[["N_100"]][["LSAM_ML"]], col.names = c("Lambda", "phi=0", "phi=0.2", "phi=0.4", "phi=0.6", "phi=0.8"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Phi values" = 5)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 1b: Absolute Bias of LSAM-ML for N=100" = 6), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

kbl(bias_ci_s1b[["N_100"]][["LSAM_ULS"]], col.names = c("Lambda", "phi=0", "phi=0.2", "phi=0.4", "phi=0.6", "phi=0.8"), format = "html") %>%
  kable_styling(full_width = F, position = "left") %>%
  add_header_above(c(" " = 1, "Phi values" = 5)) %>%
  row_spec(0, bold = TRUE) %>%
  kable_classic(full_width = F) %>%
  add_header_above(c("Study 1b: Absolute Bias of LSAM-ULS for N=100" = 6), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

```
Again, the values and therefore the interpretation of LSAM-ML and LSAM-ULS are identical.

Across all conditions, both estimators were biased.
For higher values of lambda, the estimators had less absolute Bias. This difference increased for higher phi values.
For lower values of lambda, the absolute bias increased with increasing phi. 
This trend reversed for higher values of lambda, where the absolute bias decreased with higher phi values.

Additionally, there seems to be no differential effect due to N, as the interpretations between the different N-conditions are identical.
Naturally, the bias values are larger for N=50, still.

### Comparison to original paper
In contrast to the original paper by @robitzsch_comparing_2022, we computed absolute bias to avoid infinite values in some conditions.
Additionally, computing absolute instead of relative Bias solves the problem of positive and negative biases canceling each other out.
Results partly align in terms of absolute bias:
For higher values of lambda, LSAM-ML has less absolute Bias. 
This difference increases for higher phi values.
Only for low values of lambda, the bias increases for higher values of phi. 
Unlike in this study, the original paper did not find a reversal effect for higher lambda values.
Another difference is that we, unlike @robitzsch_comparing_2022, found estimates to be biased for low phi-values.
This might be due to the difference of using absolute bias, thereby avoiding cancellation of positive and negative values.
Still, our general conclusion corresponds to the original paper:
LSAM has a general negative bias. 
The bias is larger in magnitude for lower loadings and higher true factor correlations.

# Results Simulation 2: 2-factor-CFA with 1 or 2 cross-loadings

## Error, warnings and messages
First, lets check for any messages, warnings or errors:

```{r}
#| ouput: true

#Save all errors, warnings and messages
all_messages_s2 <- readRDS("SimulationResults/sim2_results_error.rds")

#errors
errors_s2 <- all_messages_s2$errors
errors_sum_s2 <- unlist(errors_s2)
length(errors_sum_s2)

#warnings
warnings_s2 <- all_messages_s2$warnings
warnings_sum_s2 <- unlist(warnings_s2)
length(warnings_sum_s2)

#messages
messages_s2 <- all_messages_s2$messages
messages_sum_s2 <- unlist(messages_s2)
length(messages_sum_s2)


```

No errors and messages were present. There were, however, 612 warnings we will investigate in detail.


### Warnings investigation
Investigating the 'warnings' object list, it became apparent that all the warnings are the same: "no non-missing arguments to min; returning Inf".

Sometimes 4-6 warnings per repetition, more than before (up to 2 per repetition)

```{r}
unique(warnings_s2)
```

As before, this warning could indicate either convergence issues or improper solutions due to small sample size in one or multiple estimators.
Again, we are unable to check for more details, but hypothesize that this occurs mainly in the standard SEM estimators.
There is, however, evidence that this occurs frequently standard SEM estimation and was one claimed advantage of SAM- over SEM-estimation [@rosseel_structural_2022].
The combined study should check this more properly to evaluate these claims thoroughly.
As the 612 estimation are around 1.5% of the 42000 estimations performed, we will continue with the interpretation as planned, as their impact is negligable.

## Bias

General interpretation: Biased if Bias estimate larger than 0.05

Interpretation layers within one condition:

- Unbiased judgement SAM vs. SEM

- ALL across increasing N

- All within one N

-----------------


### Print all tables
```{r}
#| ouput: true

# Load the relative bias results
bias_ci_s2 <- readRDS("SimulationResults/sim2_rel_bias_ci.rds")

# Loop through each condition and print the styled table
for (condition in names(bias_ci_s2)) {
  print(create_styled_table(bias_ci_s2[[condition]], condition))
  cat("\n\n")  # Add space between tables
}


```

### One negative cross-loading (1_-0.3)
```{r}

print(create_styled_table(bias_ci_s2[["1_-0.3"]], "1_-0.3"))

```


All estimators are negatively biased (i.e. relative Bias >= -0.05) across all N conditions.
Across all estimators except SEM-ULS, the Bias decreases substantially with increasing N.
For SEM-ULS, the bias stays constant across N, and is lowest by a substantial marging compared to all SAM estimators up until N=500.
For higher N, SEM-ML is the best, but the differences between all estimators are negligible
The 4 SAM estimators have nearly identical estimation results.
In relative comparison, the two SEM estimators outperform all SAM estimators in every condition of N, even though this difference is not substantive for N=1000 and higher.

Thus, SEM estimation outperforms SAM in small to moderate sample sizes, when there is one negative cross-loading.

### One positive cross-loadings (1_0.3)
```{r}

print(create_styled_table(bias_ci_s2[["1_0.3"]], "1_0.3"))

```

All estimators are biased across all N conditions.
For the two SEM-estimators, the relative Bias is always positive and shows no trend across N.
All SAM estimators have nearly identical values and increasing relative bias for increasing N.
Their bias turn from being negative for N=50-100 to positive from N=250 on.
In relative comparison, the two SEM estimators outperform all SAM estimators for N=50. For N=100-2500, the SAM estimators outperform the SEM estimators, even though the difference is unsubstantial starting from N=1000.

Thus, SEM estimation outperforms SAM in small samples (N=50), whereas SAM outperforms SEM in smaller to moderate samples (N100-500), and slightly in higher sample sizes.

### Two negative cross-loadings (2_-0.3)
```{r}

print(create_styled_table(bias_ci_s2[["2_-0.3"]], "2_-0.3"))

```
All estimators are negatively biased across all N conditions.
Across all estimators, the Bias decreases substantially with increasing N.
The 4 SAM estimators have nearly identical estimation results.
In relative comparison, the two SEM estimators substantially outperform all SAM estimators in every condition of N.

Thus, SEM estimation outperforms SAM, when there are two negative residuals.

### Two positive cross-loadings (2_0.3)
```{r}

print(create_styled_table(bias_ci_s2[["2_0.3"]], "2_0.3"))

```

All estimators are biased across N conditions.
Both SEM estimators are positively biased across all N, with no visible trend for increasing N for ULS, but a substantial increase for ML.
All SAM estimators have nearly identical values and increasing relative bias for increasing N.
They are negatively biased in N=50 and positively biased for the other N conditions. 

In relative comparison, the four SAM estimators outperform the two SEM estimators for N=50-500, even though unsubstantial for N=500. For all higher N, no estimator substantially differs from another, but the two SEM slightly outperform.

Thus, SAM outperforms SEM in small to moderate samples (N50-250).

### Summary

Again, in 1.5% of estimations, a warning referring to potential problems with convergence or improper solutions due to small sample sizes in one or multiple estimators occured. 
This should be investigated more explicitly in the joint study.

For small to moderate samples, SEM outperforms SAM in conditions with negative cross-loadings. SAM outperforms SEM in conditions with positive cross-loadings.

### Comparison with original paper

Comparing these results with the original paper, the results are  similar and can be seen as replicated. The only difference lies in the potential convergence issues in a small number of estimations. 

Besides that, we draw the same conclusions as @robitzsch_comparing_2022:
All SAM approaches appear to be generally negatively biased for small to moderate samples in the 2-factor-CFA model. 
This leads to the comparatively worse performance in the conditions with no, one or two negative residual correlations. Additionally, this negative bias corrects for positive misspecifications and makes SAM appear superior in these conditions.

## SD

General interpretation: Substantial if percentage reductions are larger than 5%

Interpretation layers within one condition:

- Unbiased judgement SAM vs. SEM

- ALL across increasing N

- All within one N

-----------------

### print all tables
```{r}
#| ouput: true

# Load the relative bias results
sd_s2 <- readRDS("SimulationResults/sim2_sd.rds")


# Loop through each condition and print the styled table
for (condition in names(sd_s2)) {
  print(create_styled_table(sd_s2[[condition]], condition))
  cat("\n\n")  # Add space between tables
}
```

### One negative cross-loading (1_-0.3)

```{r}
#| ouput: true

print(create_styled_table(sd_s2[["1_-0.3"]], "1_-0.3"))

```


Across all estimators, the standard deviation decreases with increasing N. 
There are only two substantial differences:
Substantial percentage reduction of SD up to 30.8% when comparing SEM-ML and LSAM-ML in N=50: 
Substantial percentage reduction of SD up to 16% when comparing SEM-ML and LSAM-ML in N=100:

Thus, SAM outperforms SEM in terms of SD reduction in smaller samples.

### One positive cross-loading (1_0.3)

```{r}
#| ouput: true

print(create_styled_table(sd_s2[["1_0.3"]], "1_0.3"))

```

Across all estimators, the standard deviation decreases with increasing N. 

The SAM estimators outperform both SEM estimators in N=50 with a percentage reduction of SD up to 6.4%.
ULS-SEM outperforms all SAM estimators with a percentage reduction of SD up to 14.1% in N=100, and of 12.3% in N=250 and of 8.6% in N=500.
Above that point, overall SD is very small and negligible in all estimators
No substantial differences between the two SEM and all SAM estimators starting from moderate sample sizes (N=500-2500). 

Thus, SAM outperforms SEM in terms of SD reduction in small samples, whereas SEM outperforms SAM in smaller to moderate samples.

### Two negative cross-loadings (2_-0.3)

```{r}
#| ouput: true

print(create_styled_table(sd_s2[["2_-0.3"]], "2_-0.3"))

```

Across all estimators, the standard deviation decreases with increasing N. 

The SAM estimators substantially, but decreasingly outperform both SEM estimators in samples N=50-500 with a SD percentage reduction of 10.1% remaining in N=500.
For higher N, there are no substantial differences and overall SD is very small across estimators.

Thus, SAM outperforms SEM in terms of SD reduction in small to moderate samples.

### Two positive cross-loadings (2_0.3)

```{r}
#| ouput: true

print(create_styled_table(sd_s2[["2_0.3"]], "2_0.3"))

```

Across all estimators, the standard deviation decreases with increasing N. 

Both SEM estimators outperform the SAM estimators in N=50-500 with a SD percentage reduction up to 11.7% remaining in N=500.
For higher N, there are no substantial differences.

Thus, SEM outperforms SAM in small to moderate samples.


### Summary

For negative cross-loadings, SAM tends to outperform SEM in terms of SD reduction small to moderate samples.
For positive cross-loadings, SEM tends to outperform SAM in terms of SD reduction smaller to moderate samples.
There seems to be an effect due to the amount of misspecification, such that the conditions with 2 cross-loadings had larger differences in performance (in both directions).
This could be interesting to keep in mind for higher numbers of misspecification (Study4?).

## RMSE
differences large than 0.03, we will report, without drawing a substantive conclusion.

Interpretation layers within one condition:

- ALL across increasing N

- All within one N

-----------------

###Print all tables
```{r}
rmse_s2 <- readRDS("SimulationResults/sim2_rmse.rds")

```

### One negative cross-loading (1_-0.3)

```{r}
#| ouput: true

print(create_styled_table(rmse_s2[["1_-0.3"]], "1_-0.3"))

```

Across all estimators, the RMSE decreases with increasing N.
For small to moderate samples (N=50-500), both SEM estimators were more efficient than all SAM estimators.
For higher N, there are no substantial differences present.

### One positive cross-loading (1_0.3)

```{r}
#| ouput: true

print(create_styled_table(rmse_s2[["1_0.3"]], "1_0.3"))

```

Across all estimators, the RMSE decreases with increasing N.
For smaller and large samples (N=50-100 and N=1000-100000), none of the estimators differed from another.
For N=250, all 4 SAM estimators substantially outperformed the two SEM estimators.
For N=500, all 4 SAM estimators substantially outperformed SEM-ML estimation.

### Two negative cross-loadings (2_-0.3)

```{r}
#| ouput: true

print(create_styled_table(rmse_s2[["2_-0.3"]], "2_-0.3"))

```

Across all estimators, the RMSE decreases with increasing N.
For small samples with N=50, none of the estimators differed in terms of efficiency.
For all other conditions, The two SEM estimators were more efficient than all SAM estimators.

### Two positive cross-loadings (2_0.3)

```{r}
#| ouput: true

print(create_styled_table(rmse_s2[["2_0.3"]], "2_0.3"))

```

Only for the two SEM estimators, the RMSE decreases with increasing N, but remains at higher values between 0.22-0.25 for all estimators.

For small to moderate samples (N=50-250), all SAM estimators were more efficient than the two SEM estimators.
For higher N, there are no substantial differences present.

### Summary

In moderate samples, SAM generally outperformed SEM in terms of RMSE.
The only exception is the condition with 1 negative cross-loading, where SEM outperformed SAM in small to moderate samples.
In the condition with 2 positive cross-loadings, SAM was more efficient in small samples as well.
Unlike in Study 1, there seems to be no effect due to amount of misspecification.

## Summary Study 2

We gained the following insights with regards to the 2-factor-CFA model:
Unlike the original paper by @robitzsch_comparing_2022, we experienced some issues that are related to convergence or improper solutions in small samples.

In terms of bias in small to moderate samples, SEM outperformed SAM in conditions with negative cross-loadings. 
SAM appeared to outperform SEM in conditions with positive cross-loadings. 
This, however, is again an artifact of its negative small sample bias and not its performance. 
The results align with the ones obtained by the original paper.

With regards to SD reduction, findings were different. 
In conditions with negative cross-loadings, SAM tended to outperform SEM, wheras the opposite was true for conditions with positive cross-loadings.
Here, an effect due to the amount of misspecification seemed to be present, like for RMSE in Study 1.

In terms of RMSE, results differed again. 
Generally, SAM was more efficient than SEM in moderate samples. 
For small samples, the results were not as clear.
Unlike in Study 1, there seems to be no effect due to amount of misspecification.

Overall, we can infer that findings are somewhat incoherent and it can not be stated that one estimation method is generally performing better than the other, based on the results of Study 2.

# Results simulation 3:2-factor-CFA with one cross-loading and one residual correlation

## Error, warnings and messages
First, lets check for any messages, warnings or errors:

```{r}
#| ouput: true

#Save all errors, warnings and messages
all_messages_s3 <- readRDS("SimulationResults/sim3_results_error.rds")

#errors
errors_s3 <- all_messages_s3$errors
errors_sum_s3 <- unlist(errors_s3)
length(errors_sum_s3)

#warnings
warnings_s3 <- all_messages_s3$warnings
warnings_sum_s3 <- unlist(warnings_s3)
length(warnings_sum_s3)

#messages
messages_s3 <- all_messages_s3$messages
messages_sum_s3 <- unlist(messages_s3)
length(messages_sum_s3)


```

No errors and messages were present. There were, however, 193 warnings we will investigate in detail.

### Warnings investigation
Investigating the 'warnings' object list, it became apparent that all the warnings are the same: "no non-missing arguments to min; returning Inf".

```{r}
unique(warnings_s3)
```

As before, this warning could indicate either convergence issues or improper solutions due to small sample size in one or multiple estimators.
Again, we are unable to check for more details, but hypothesize that this occurs mainly in the standard SEM estimators.
There is, however, evidence that this occurs frequently standard SEM estimation and was one claimed advantage of SAM- over SEM-estimation [@rosseel_structural_2022].
The combined study should check this more properly to evaluate these claims thoroughly.
As the 193 estimation are around 1.8% of the 10500 estimations performed, we will continue with the interpretation as planned, as their impact is negligable.

## Bias

```{r}
#| ouput: true

# Load the relative bias results
bias_ci_s3 <- readRDS("SimulationResults/sim3_rel_bias_ci.rds")

#Format table for results
kbl(bias_ci_s3, col.names = c("Method/Metric", "50", "100", "250", "500", "1000", "2500", "1e+05"), format = "html") %>%
    kable_styling(full_width = F, position = "left") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Study 3:" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

```

All estimators were biased for every condition of N.
The SAM estimators had nearly identical values.
Besides SEM-ULS, all estimators' Bias increased substantially, even though the increase was stronger for the SAM estimators.
Only the SAM estimators had negative Bias for N=50-100.
For N=50, none of the estimators outperformed another.
For N=100-500, all SAM estimators outperformed the two SEM estimators.
For higher N, none of the estimators substantially differed from another.

Thus, all SAM estimators outperformed the two SEM estimators in smaller to moderate samples.

### Comparison with original paper

Comparing these results with the original paper, our results were somewhat different.
Even though the results were similar for most conditions (@robitzsch_comparing_2022 had the same results in conditions N=100-250 and N=1000-100000), the results were different for N=500.
This, combined with the findings from the additional condition of N=50 revealed that SAM estimation outperformed SEM estimation in this data generating scenario in small to moderate samples.
Another difference lies, again, in the potential convergence issues in a small number of estimations. 

## SD

```{r}
#| ouput: true

metrics_s3 <- readRDS("SimulationResults/sim3_metrics_list.rds")

# Load the SD results
sd_s3 <- metrics_s3[["sd"]]

kbl(sd_s3, col.names = c("Method/Metric", "50", "100", "250", "500", "1000", "2500", "1e+05"), format = "html") %>%
    kable_styling(full_width = F, position = "left") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Study 3:" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

```

Across all estimators, the standard deviation decreases with increasing N. 
There are thre substantial differences:
Substantial percentage reduction of SD up to 5.5% in favor of SEM-ULS when comparing with GSAM-ULS in N=50. 
Substantial percentage reduction of SD up to 21.7% when comparing SEM-ULS when comparing with GSAM-ULS in N=100.
Substantial percentage reduction of SD up to 15.6% when comparing SEM-ULS when comparing with GSAM-ULS in N=250.
For higher N, the SD still partly differs but becomes negligibly small.

Thus, SEM outperforms SAM in terms of SD reduction in small to moderate samples.

## RMSE

```{r}
#| ouput: true

# Load the rmse results
rmse_s3 <-  metrics_s3[["rmse"]]

#Format table for results
kbl(rmse_s3, col.names = c("Method/Metric", "50", "100", "250", "500", "1000", "2500", "1e+05"), format = "html") %>%
    kable_styling(full_width = F, position = "left") %>%
    add_header_above(c(" " = 1, "Sample Size" = 7)) %>%
    row_spec(0, bold = TRUE) %>%
    kable_classic(full_width = F) %>%
    add_header_above(c("Study 3:" = 8), bold = TRUE, align = "left", line = TRUE, italic = TRUE)

```

differences large than 0.03, we will report, without drawing a substantive conclusion.

Interpretation layers within one condition:

- ALL across increasing N

- All within one N


Across all estimators, the RMSE decreased with increasing N.
For N=50 and N=1000-100000, none of the estimators differed from another.
For N=100-500, the two SEM-estimators outperform all SAM-estimators in terms of efficiency.

Thus, SEM outperforms SAM in smaller to moderate samples.

## Summary Study 3
As before, we experienced some issues that are related to convergence or improper solutions in small samples.

In terms of Bias, all SAM estimators outperformed the two SEM estimators in smaller to moderate samples.

These results differed from the ones obtained with regards to SD and RMSE, where SEM tended to outperform SAM in smaller to moderate samples.

Importantly, one has to bear in mind that same as before, SAM only appears to be more robust against positive misspecifications due to its general negative bias in small samples. 
As in this study, same as in @robitzsch_comparing_2022, the misspecification values were positive, we again cannot conclude that SAM performs better than SEM.









----------------------------------------
# Summary 2-factor-CFA models
SAM does not outperform! Negative small sample bias

## Study 1
In terms of bias in small to moderate samples, SEM outperforms SAM in conditions with no, one and two negative residual correlations. SAM appears to outperform SEM for positive residual correlations. This, however, is an artifact of its negative small sample bias and not its performance. The results align with the ones obtained by the original paper.

With regards to SD and RMSE, results are incoherent with no clear pattern emerging. 

Thus, it cannot be stated that one estimation method is generally performing better than the other, based on the results of Study 1.

##Study 2
Overall, we can infer that findings are somewhat incoherent and it can not be stated that one estimation method is generally performing better than the other, based on the results of Study 2.



##Study 3
In terms of Bias, all SAM estimators outperformed the two SEM estimators in smaller to moderate samples.

These results differed from the ones obtained with regards to SD and RMSE, where SEM tended to outperform SAM in smaller to moderate samples.

Importantly, one has to bear in mind that same as before, SAM only appears to be more robust against positive misspecifications due to its general negative bias in small samples. 
As in this study, same as in @robitzsch_comparing_2022, the misspecification values were positive, we again cannot conclude that SAM performs better than SEM.





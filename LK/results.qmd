---
title: "Adversarial Simulation"
subtitle: "A case study"
author: "Leonard Kosanke"
format:
    pdf:
        include-in-header: ../preamble.tex
        fontsize: 11pt
        linestretch: 1.2
        geometry: "left=4cm, right=4cm, top=3cm, bottom=5cm, bindingoffset=7mm"
        classoption: twoside
        papersize: a4
fontsize: 11pt
# https://practicaltypography.com/page-margins.html
# 3.81 to 5.08cm or 45-90 characters or 2-3 alphabets
# asymetric geometry is set after/in frontmatter.md
bibliography: ../bibliography.bib
engine: knitr
---

The git hash is: `{r} suppressWarnings(system2("git", c("rev-parse", "--short=5", "HEAD"), stdout = TRUE, stderr = TRUE))`

# Deviations from the simulation protocol

## Studies 1,2,3

For all these studies, 3 deviations are present.
Firstly, the total number of conditions turned out to be higher, as the calculation in the protocol forgot the addition of N=50 to be added to the factor sample size.
Secondly, It was not detailed what type of LSAM estimation will be implemented. For completeness sake, we included ML- as well as ULS-LSAM estimation.
Lastly, instead of calculating the relative bias as originally planned, we calculated the absolute bias to mitigate biases of opposite directions canceling each other out, as rightly pointed out in study 4 of @robitzsch_comparing_2022.

## Study 1b

In Study 1b, contrary to the simulation protocol, we added another sample size with N=50 to get a more nuanced understanding of the small-sample bias of LSAM. 
For the same reasons as in studies 1,2 and 3, we used ML as well as ULS LSAM estimation and calculated the absolute bias again.

## Study 4

Again, I forgot the addition of N=50 in the original condition calculation.

## Study 4a

I decided to include a variation of study 4a after all, as the relation between beta/phi values and N is interesting: 
The original paper showed that results differed between estimators, depending on the size of the factor correlation in Studies 5, 6 and 1b, as well as differential effects due to N in Study 1b.
We deemed it interesting to investigate these results in a model with a different number of factors.
These findings, combined with our goal to investigate realistic data conditions, rather than just the population level, left us to include Study 4a, with the addition of an additional factor sample size.
As DGM1 neither contained misspecification (which is central to our research question), nor did it lead to interesting results in the original study 4, it was omitted.
Similarly, as no substantial conclusions were drawn with regards to RMSE in the original papers study 1 and 2, we omitted its calculation as well.

## Studies 5,6

Similar to the reasoning in study 4a, we again included N as a factor to more closely align to our research interest in realistic data conditions. 
Thus, we also reverted to the previous analysis of results, not implementing the “SAM vs. SEM better” approach that was preregistered. 
Hence, the number of conditions deviated in the analyses as well.
Additionally, we doubled the interval length between the phi/beta-conditions to reduce the high number of results that would be present. As there were no interesting findings in the original differentiation, we deemed this to be a fair, pragmatic deviation to reduce complexity.

# Result analysis

## Original paper

### Bias
In the original paper, @robitzsch_comparing_2022 had some inconsistencies when it came to the interpretation of the results of their simulation studies. 
They did not thoroughly define an a prior criteria for when an estimator outperformed another. 
Only in their Studies 5 and 6, they defined an absolute bias equal to or larger than 0.05 as relevant. 
Additionally, when an estimator had a minimum percentage error of 20% higher than another, they defined it as being better than the other, in these studies.
Across the other studies, interpretations sometimes varied. 
In some studies (e.g. Study 1), they seemed to orient on the same value of 0.05, but this time as an absolute difference value, without the addition of a certain percentage cut-off. 
In other studies (e.g. Study 2), they seemed to deviate from this value to define relevant difference in absolute bias.
Importantly, this deviation was present in both directions, and therefore did not appear to be consciously biased towards one direction of possible outcomes.
In consequence, it appears that interpretations aren't coherent across the studies.

Another important aspect to highlight in the original papers results refers to the general presence of a negative bias in LSAM estimation. 
Even though this bias is correctly identified and explained already in Study 1, @robitzsch_comparing_2022 conduct multiple follow-up studies only investigating positive values in the misspecifications, namely studies 3-6. 
This is done without a general note of some sorts, that this bias should apply in these data-generating scenarios the same as in Studies 1 and 2.
In consequence, the interpretations @robitzsch_comparing_2022 make, are somewhat misleading on first sight, if one does not account for this bias.
This does not, however, explain why they did not include more studies or conditions with negatively valenced residual correlations or cross-loadings. We hypothesize, that the goal of the paper was to show that even in these favorable constellations, SAM does not perform better than SEM.

To be fair, it should be stated that most of the final conclusions drawn in the paper are correct, anyway. 
With the use of the term "generally", @robitzsch_comparing_2022 are right in that there are many data constellations, were SAM does not outperform traditional SEM.
They also acknowledge, that for non-saturated structural models, SAM seems to be the better choice.

For our studies, in order to have a more coherent interpretation of results, we will define an estimator to outperform another, based on the cut-off of 0.05 for bias.

### SD and RMSE

Even though @robitzsch_comparing_2022 report tables with results for SD and RMSE, they rarely explicitly mentioned them in the results sections or give cut-offs for substantial relevance.

SD is only reported and mentioned in Study 1, even though also claimed to be calculated in studies 2 and 3. 
In this study, they mentioned a 6.6% reduction of SD for LSAM in comparison to ML. 
This calculation, however, is wrong as the reduction amounts to 8%.
For our interpretation, we will report percentage reductions larger than 5% as well and compare it with the original study, without drawing a substantive conclusion.

RMSE (and average RMSE) is reported in Studies 1-4, but only in study 2 and 4 explicitly interpreted for our estimators of interest.
In study 2, differences of up to 0.03 are present and interpreted as not being substantial differences.
In study 4, differences of up to 0.04 are present and interpreted as not being substantial differences.
We infer that this lack of mentioning can be interpreted as there being no differences of interest. 
Thus, we expect our studies to yield similar results. 
If, however, there are differences large than 0.03, we will report them, without drawing a substantive conclusion.


# Open questions

## 1. Bias and RMSE calculation
How to compute Bias and RMSE, especially if more than one phi is present (3- and 5-factor models from Studies 4-6)? 

The Paper says it is preferrable to use Average absolute bias (AvgAbsBias) over average relative bias, but they do not use it up until study 4. Following their reasoning, I have calculated the AvgAbsBias starting from Study 1.

**Calculation Study 1-3:**

AvgAbsBias: mean(abs(phi_estimates - true_phi)) 

RMSE: sqrt(mean((phi_estimates - true_phi)^2))

Alternatively, relative bias could be: mean(estimates - true_values) / true_value

true value nur innerhalb eines phi, wenn die phis alle den gleichen Wert haben, dann müssen sie getrennt sein

**Calculation Study 4-6:**

AvgAbsBias:

1. Created list of absolute biases with abs(estimates_list[[phi]] - true_phis[phi]) for all phis within one condition.
2. Took mean of all these across repetitions

AvgRMSE:

1. Calculated squared error within each condition and repetition
2. Took mean across repetitions

*But: Unclear what exactly AvgAbsBias is for multiple phis (=more than 2 factors in the model)*

- Do we Either:
    1. calculate a sum of AbsBias for each estimated model (so within each condition and repetition)
    2. Take the average across repetitions for each conditions
    
- Or:
    Simply calculate the Avg of ALL AbsBiases, across repetitions for each condition (so without calculating a sum for each estimated model, as I did up to now).
    
Bottom line question: How should I calculate them?


Answer: both is valid: If we want 0, then absolute bias is preferrable, if true value is not 0 (which it)
Ideally, beides reporten, eines interpretieren
AvgAbsBias: Mein weg ist korrekt

Reporting: STandardfehler/ KI für Bias berichten


## 2. How to calculate the phi values like in the paper? (Simulations 4 & 4a): 

In the paper, they say they want to improve upon Rosseel and Loh by investigating “marginal factor correlations” instead of “condition associations”.
They then calculate the phi values based on the size of the regression coefficient beta = 0.1. 
But it is unclear how they calculate them, and why they come up with 10 instead of 8 phi values (In the original paper, Rosseel and Loh only compute 8, corresponding to the number of regressions).

For study 4, I took his 10 values and used them, but need to calculate them for study 4a (for now in study 4a, I have only taken the unstandardized regression weights, as in the original Rosseel and Loh paper). I specified covariance for the 2 of the 10 values that are not regressions (phi21 and phi52) —> Should they also be regressions?

Answer:
take lavaan: implicit cov. matrix berechnen lassen, wie in RL



## 3. How to do the matrix algebra? I don’t get it

How do I ideally calculate the covariance matrix? This is done differently for different studies in the original paper. I used what I had without really understanding why. For example, why S is calculated differently in Studies 1-3 vs. 5-6. Might it be because the analyses in Robitzsch studies 5 and 6 were only conducted on the population level?

What I did at the moment: 

Studies 1-3 like in Robitzsch:

```{r}
#| eval: false

S <- LAM %*% PHI %*% t(LAM) + THETA
dat <- MASS::mvrnorm(n = N, mu = rep(0, 6), Sigma = S)
```

Study 4: imported true model cov matrix from Robitzsch and used them as Sigma

Study 4a: like in Rosseel and Loh paper (with functions taken from OSF), as no info was provided by Robitzsch

Study 5-6: Like Robitzsch but probably wrong matrix setup as calculation was done on population level only in Robitzsch, unlike me:

```{r}
#| eval: false
  
TR <- LAM %*% PHI %*% t(LAM)
PSI <- matrix(0, nrow=9, ncol=9)
diag(PSI) <- 1 - diag(TR)
S <- TR + PSI  
dat <- MASS::mvrnorm(n = N, mu = rep(0, nrow(LAM)), Sigma = S, empirical = TRUE) ####empirical=true ist wichtiger unterschied, daten = exact covariance, kein sampling fehler
```
###### Alles in lavaan spezifizieren und cov-matrix bekommen

For context, Robitzsch does the following within his simulation loop in Study 1: 

```{r}
#| eval: false

S0 <- S <- LAM %*% PHI %*% t(LAM) + THETA
rownames(S) <- colnames(S) <- c( paste0("X",1:3), paste0("Y",1:3) )

if (N<inf_val){
    dat <- MASS::mvrnorm(n=N, mu=rep(0,6), Sigma=S)
    S <- cov(dat) * (N-1) / N
}

# save example datasets
save.data( S , paste0("DATA_DES",dd ) , type="csv2" , path= pf1, row.names=TRUE)
```

And then later on estimates the models as below:

```{r}
#| eval: false

mod1a <- lavaan::sem(lavmodel, sample.cov=S, sample.nobs=1e5, estimator="ML", std.lv=std.lv)
```



## 4. How to handle warnings in studies 3 & 4

Visible in the console after running the simulation. I commented my opinion below the respective code chunks.


For study 3, relevant error!

In purr: insistently() = versuchs nochmal, wenn es einen Fehler gibt oder possibly()quietly()safely() zusammen = alle warnings/messages werden gespeichert (safely) und wird ausgegeben wenn es etwas gibt, statt dem eigentlich value, ohne die simulation abzubrechen
safely() auf jeden fall anwenden auf simulation() function
wenn geht in kombination mit quietly()
possibly() ohne abzubrechen Fehler speichern, am Ende
--> Alle angucken und kombinieren, eventuell safely und quietly damit man ergebnisse, warnings, fehler alles gemeinsam ausgegeben bekommen
Immer die gleiche Funktion nutzen

## 5. Results setup

How should I index into the single simulation results in this document?
Running all the scripts in the final markdown with source() is not feasible, as it would repeat the simulation.
The GPT suggestion to split up report_analysis() functions in seperate scripts would lead to double the amount of scripts and is also inadvisable. 
Addtionally, executing quarto render would lead to running all simulations again, as of now.
As of now, it fails anyway due to titlesec problems (error-prone part in the preamble).
My idea: we would run the simulation on the cluster, and export all tables from report_analysis(). These can be indexed in the results.qmd for analysis. This way, the simulation only needs to be run once and is not part of quarto render.


Möglichst auf einzelne Simulation beziehen interpretation

Durchführung: caching von rmarkdown (wenn sich nichts geändert hat, rechne nichts neu),

Wenn downloaden der Ergebnisse: Dann in einem Mechanismus, idealerweise ausführen mit Makefile!
Sind die einzelnen Skripte komplett unabhängig? Alle in eigenem environment laufen lassen dann gut! source(environment), dieses argument für jede sim anlegen, eigene envrironments
rmarkdown: spin

knitr:: purl sollte gehen in quarto: konvertiert markdown in script https://quarto.org/docs/reference/cells/cells-knitr.html

Noch einmal das Repo: https://github.com/aaronpeikert/varynfit/blob/master/varynfit.Rmd angucken, er lädt

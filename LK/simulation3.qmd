# Script for Simulation Study 3

```{r}
set.seed(1)
```

# Packages

Copied and pasted from original paper.

```{r}
# Specify the libraries to load
libraries <- c("GPArotation", "CDM", "miceadds", "TAM", "sirt", "lavaan", "dplyr", "tidyr", "purrr", "tidyverse", "furrr")
# Set the R mirror to the cloud mirror of RStudio
options(repos = "https://cloud.r-project.org/")

# Load the libraries
for (library_name in libraries) {
  if (!require(library_name, character.only = TRUE)) {
    install.packages(library_name)
    library(library_name, character.only = TRUE)
  }
}
```

# Specify 2-factor-Model

```{r}
model <- "

#Structural part
    FX =~ l1*X1 + l2*X2 + l3*X3
    FY =~ l4*Y1 + l5*Y2 + l6*Y3
    FX ~~ 1*FX    #Fixed latent variance
    FY ~~ 1*FY    #Fixed latent variance
    FX ~~ phi*FY
    phi < 0.99    #Phi between -1 and 1
    phi > -0.99

#Measurement part    
    X1 ~~ vX1*X1
    X2 ~~ vX2*X2    
    X3 ~~ vX3*X3
    Y1 ~~ vY1*Y1
    Y2 ~~ vY2*Y2    
    Y3 ~~ vY3*Y3
    l1 > 0.01     #only positive loadings
    l2 > 0.01    
    l3 > 0.01
    l4 > 0.01
    l5 > 0.01
    l6 > 0.01       
    vX1 > 0.01    #only positive variances
    vX2 > 0.01    
    vX3 > 0.01
    vY1 > 0.01
    vY2 > 0.01    
    vY3 > 0.01   
    "
```

# Setup and Design

```{r}
setup_design <- function() {
  # Sample sizes
  N_sizes <- c(50, 100, 250, 500, 1000, 2500, 10^5)
  
  design <- data.frame(N_sizes = N_sizes)
  
  return(design)
}


```

# Data generating Mechanism

## Fixed values
```{r}
lam1 <- 0.55
lam2 <- 0.45
phi <- 0.60 

# Base loading matrix for two factors
LAM <- matrix(0, nrow=6, ncol=2)
LAM[1:3, 1] <- lam1  
LAM[4:6, 2] <- lam2 

LAM[1, 2] <- 0.3  # Cross-loading deltax1

# Base correlation matrix for factors
PHI <- matrix(0, nrow=2, ncol=2)
diag(PHI) <- 1
PHI[1, 2] <- PHI[2, 1] <- phi

# Base unique variances for observed variables
THETA <- diag(c(rep(1 - lam1^2, 3), rep(1 - lam2^2, 3)))

THETA[2, 5] <- THETA[5, 2] <- 0.12  # Correlated residuals for X2 and Y2

```

## Varying values
None

# Apply Syntax
```{r}
pop.model <- paste(
  
"#Structural part",
    paste("FX =~", LAM[1,1], "*X1 +", LAM[2,1], "*X2 +", LAM[3,1], "*X3"),
    paste("FY =~", LAM[4,2], "*Y1 +", LAM[5,2], "*Y2 +", LAM[6,2], "*Y3 +", LAM[1, 2], "*X1"), # Cross-loading
    "FX ~~ 1*FX",    
    "FY ~~ 1*FY",    
    paste("FX ~~", PHI[1,2], "*FY"),
"#Measurement part",    
    paste("X1 ~~", THETA[1,1], "*X1"),
    paste("X2 ~~", THETA[2,2], "*X2"),
    paste("X3 ~~", THETA[3,3], "*X3"),
    paste("Y1 ~~", THETA[4,4], "*Y1"),
    paste("Y2 ~~", THETA[5,5], "*Y2"),
    paste("Y3 ~~", THETA[6,6], "*Y3"),
"Correlated residual",
    paste("X2 ~~", THETA[2,5], "*Y2"),
    sep = "\n"
  ) 

  
cat(pop.model)  
  
  
```


# Simulate data
```{r}
simulate_data <- function(N) {
  
  df_dat <- simulateData(pop.model, sample.nobs = N)
  
  return(df_dat)
}


```

# Planned Analysis
```{r}
#Specify estimation methods of interest

estimators <- list(
  SEM_ML = \(d) lavaan::sem(model, data=d, estimator="ML", std.lv= TRUE),
  SEM_ULS = \(d) lavaan::sem(model, data=d, estimator="ULS", std.lv= TRUE),
  LSAM_ML = \(d) lavaan::sam(model, data=d, sam.method="local", estimator = "ML", std.lv= TRUE),
  LSAM_ULS = \(d)lavaan::sam(model, data=d, sam.method="local", estimator = "ULS", std.lv= TRUE),
  GSAM_ML = \(d) lavaan::sam(model, data=d, sam.method = "global", estimator = "ML", std.lv= TRUE),
  GSAM_ULS = \(d) lavaan::sam(model, data=d, sam.method = "global", estimator = "ULS", std.lv= TRUE)
)
# postprocess each model output
estimators <- modify(estimators, ~compose(\(e)filter(e, label == "phi")$est, parameterEstimates, .))
# apply all estimators to the same dataset
apply_estimators <- \(d) map(estimators, exec, d)

planned_analysis <- compose(apply_estimators, simulate_data)

#Args of planned_analysis() = Args of simulate_data()
planned_analysis(100)
```

# Extract results

```{r}
extract_results <- function(results_df_raw){
#Compute performance measures
results_metrics <- results_df_raw %>%
    group_by(N) %>%
    summarize(across(everything(),
       list(
          abs_bias = ~mean(abs(.x - phi)),              # Average absolute bias
          rel_bias = ~mean(.x - phi) / phi,             # Relative bias
          sd = ~sd(.x),                                 # Standard deviation of estimates
          rmse = ~sqrt(mean((.x - phi)^2)),             # Root mean square error
          se_bias = ~(sd(abs(.x - phi)))/ sqrt(unique(N)),      # SE of bias
          ci_lower = ~(mean(abs(.x - phi)) - qt(0.975, df = unique(N) - 1) * (sd(abs(.x - phi)))/ sqrt(unique(N))),  # Lower CI
          ci_upper = ~(mean(abs(.x - phi)) + qt(0.975, df = unique(N) - 1) * (sd(abs(.x - phi)))/ sqrt(unique(N)))   # Upper CI
                    )),  .groups = 'drop')

  # Split metrics by rc and psi_value into separate lists
  split_metrics <- results_metrics %>%
    group_by(rc, psi_value) %>%
    group_split() %>%
    set_names(map(., ~paste(unique(.x$rc), unique(.x$psi_value), sep="_")))

  # Define a function to transform each group into the desired format
  transform_group <- function(df_group) {
    df_group <- df_group %>%
      select(-rc, -psi_value) %>%
      pivot_longer(cols = starts_with(c("SEM_","LSAM_","GSAM_")), names_to = "method_metric", values_to = "value")

    # Creating nested lists for each metric
    list(
      abs_bias = df_group %>% filter(str_detect(method_metric, "abs_bias")) %>%
              pivot_wider(names_from = N, values_from = value),
      rel_bias = df_group %>% filter(str_detect(method_metric, "rel_bias")) %>%
                 pivot_wider(names_from = N, values_from = value),
      sd = df_group %>% filter(str_detect(method_metric, "sd")) %>%
              pivot_wider(names_from = N, values_from = value),
      rmse = df_group %>% filter(str_detect(method_metric, "rmse")) %>%
              pivot_wider(names_from = N, values_from = value),
      se_bias = df_group %>% filter(str_detect(method_metric, "se_bias")) %>%
                pivot_wider(names_from = N, values_from = value),
      ci_lower = df_group %>% filter(str_detect(method_metric, "ci_lower")) %>%
                pivot_wider(names_from = N, values_from = value),
      ci_upper = df_group %>% filter(str_detect(method_metric, "ci_upper")) %>%
                pivot_wider(names_from = N, values_from = value)
    )
  }

  # Apply the transformation to each group and store the results
  metrics_list <- map(split_metrics, transform_group)

  return(metrics_list)
}
```


#  Simulation Study
```{r}
simulation_study <- function(design, model, num_repetitions , true_phi, models) {
  # Initialize dataframe
  results_df <- data.frame(
    ModelType = character(),
    N = integer(),
    AvgAbsBias = numeric(),
    SDPhi = numeric(),
    RMSEPhi = numeric(),
    stringsAsFactors = FALSE
  )
  
  for(model_type in models) {
    for(i in 1:nrow(design)) {
      N_size <- design$N_sizes[i]
      phi_estimates <- replicate(num_repetitions, {
        planned_analysis(N_size, model, model_type)
      })
      
      # Compute the metrics directly
      avg_abs_bias <- mean(abs(phi_estimates - true_phi))
      sd_phi <- sd(phi_estimates)
      rmse_phi <- sqrt(mean((phi_estimates - true_phi)^2))
      
      # Combine the results into a single row
      results_row <- data.frame(
        ModelType = model_type,
        N = N_size,
        AvgAbsBias = avg_abs_bias,
        SDPhi = sd_phi,
        RMSEPhi = rmse_phi
      )
      
      # Bind the row to the results dataframe
      results_df <- rbind(results_df, results_row)
    }
  }
  
  return(results_df)
}




```

## RUN
```{r}
design <- setup_design()
true_phi <- 0.6  # True value of phi
models <- c("SEM_ML", "SEM_ULS", "LSAM_ML", "LSAM_ULS", "GSAM_ML", "GSAM_ULS")  # Models to test
num_repetitions <- 2

# Execute the simulation study
results_df <- simulation_study(design, model, num_repetitions, true_phi, models)

# View the results
print(results_df)


```
Might there be NA's somewhere that result in the warning? Nothing visible in the results.


# Report analysis
```{r}
report_analysis <- function(results_df, models) {
  # Define the list to store the tables
  tables_list <- list()

  # Define metric names to iterate over
  metrics <- c("Bias", "SD", "RMSE")
  
  subset_df <- results_df %>%
        mutate(ModelType = factor(ModelType, levels = models)) # Ensure ModelType order

  # Loop through each metric to pivot and create tables
  for (metric in metrics) {
    metric_colnames <- grep(metric, names(subset_df), value = TRUE)
    
    # Pivot the dataframe to a wide format for the current metric and reorder rows
    wide_df <- subset_df %>%
      select(ModelType, N, all_of(metric_colnames)) %>%
      arrange(ModelType) %>% # Arrange rows by ModelType
      pivot_longer(cols = all_of(metric_colnames), names_to = "Metric", values_to = "Value") %>%
      mutate(Value = round(Value, 2)) %>% # Round the values
      pivot_wider(names_from = N, values_from = Value) %>%
      select(-Metric)
    
    # Generate the table name based on the metric
    table_name <- paste(metric)
    
    # Store the table in the list with the corrected naming
    tables_list[[table_name]] <- wide_df
  }

  
  return(tables_list)
}

list_of_tables <- report_analysis(results_df, models)

# Access a specific table
bias_table <- list_of_tables[["Bias"]] # Example for Bias

#display all tables
report_analysis(results_df, models)
```



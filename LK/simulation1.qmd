# First script for Simulation Study 1

Used for writing or coding?


```{r}
set.seed(1)
```



# Packages

Copied and pasted from original paper.

```{r}
# Specify the libraries to load
libraries <- c("GPArotation", "CDM", "miceadds", "TAM", "sirt", "lavaan", "dplyr", "tidyr", "purrr", "tidyverse", "furrr")
# Set the R mirror to the cloud mirror of RStudio
options(repos = "https://cloud.r-project.org/")

# Load the libraries
for (library_name in libraries) {
  if (!require(library_name, character.only = TRUE)) {
    install.packages(library_name)
    library(library_name, character.only = TRUE)
  }
}
```

# Source relevant R Functions

# Specify 2-factor-Model
```{r}
model <- "

#Structural part
    FX =~ l1*X1 + l2*X2 + l3*X3
    FY =~ l4*Y1 + l5*Y2 + l6*Y3
    FX ~~ 1*FX    #Fixed latent variance
    FY ~~ 1*FY    #Fixed latent variance
    FX ~~ phi*FY
    phi < 0.99    #Phi between -1 and 1
    phi > -0.99

#Measurement part    
    X1 ~~ vX1*X1
    X2 ~~ vX2*X2    
    X3 ~~ vX3*X3
    Y1 ~~ vY1*Y1
    Y2 ~~ vY2*Y2    
    Y3 ~~ vY3*Y3
    l1 > 0.01     #only positive loadings
    l2 > 0.01    
    l3 > 0.01
    l4 > 0.01
    l5 > 0.01
    l6 > 0.01       
    vX1 > 0.01    #only positive variances
    vX2 > 0.01    
    vX3 > 0.01
    vY1 > 0.01
    vY2 > 0.01    
    vY3 > 0.01   
    "
```

# Setup and Design

```{r}
setup_design <- function() {
  # Sample sizes
  N_sizes <- c(50, 100, 250, 500, 1000, 2500, 10^5)
  
  # Misspecification conditions: (0, 1, 2) correlated residuals, and psi values
  rc_conditions <- c(0, 1, 2)
  psi_values <- c(0.12, -0.12)
  
  # Expand grid to create a data frame of all combinations
  design <- expand.grid(N = N_sizes, rc = rc_conditions, psi_value = psi_values)
  
  # Adjust for the condition where rc is 0 (correlated residuals are not considered)
  design <- design[!(design$rc == 0 & design$psi_value != 0.12), ]
  
  return(design)
}

```

# Data generating Mechanism

## Fixed values
```{r}
lam1 <- 0.55
lam2 <- 0.45
phi <- 0.60

LAM <- matrix(0, nrow=6, ncol=2)
LAM[1:3, 1] <- lam1
LAM[4:6, 2] <- lam2

PHI <- matrix(0, nrow=2, ncol=2)
diag(PHI) <- 1
PHI[1, 2] <- PHI[2, 1] <- phi

THETA <- diag(c(rep(1-lam1^2, 3), rep(1-lam2^2, 3)))

```

## Varying values
```{r}
get_dgm <- function(rc, psi_value) {
  
  # Adjust THETA for misspecification
  if (rc >= 1) {
    THETA[1, 4] <- THETA[4, 1] <- psi_value
  }
  if (rc == 2) {
    THETA[2, 5] <- THETA[5, 2] <- psi_value
  }
  
  return(list(LAM = LAM, PHI = PHI, THETA = THETA))
}

```

# Simulate data
```{r}
simulate_data <- function(N, rc, psi_value) {
  # Get DGM parameters
  dgm_params <- get_dgm(rc, psi_value)
  THETA <- dgm_params$THETA
  
  S <- LAM %*% PHI %*% t(LAM) + THETA
  rownames(S) <- colnames(S) <- c(paste0("X", 1:3), paste0("Y", 1:3))
  
  # Generate data
  dat <- MASS::mvrnorm(n = N, mu = rep(0, 6), Sigma = S)
  df_dat <- as.data.frame(dat)
  names(df_dat) <- c("X1", "X2", "X3", "Y1", "Y2", "Y3")
  
  return(df_dat)
}

```

# Planned Analysis
```{r}
#Specify estimation methods of interest

models <- list(
  SEM_ML = \(d) lavaan::sem(model, data=d, estimator="ML", std.lv= TRUE),
  SEM_ULS = \(d) lavaan::sem(model, data=d, estimator="ULS", std.lv= TRUE),
  LSAM_ML = \(d)
  lavaan::sam(model, data=d, sam.method="local", estimator = "ML", std.lv= TRUE),
  LSAM_ULS = \(d)lavaan::sam(model, data=d, sam.method="local", estimator = "ULS", std.lv= TRUE),
  GSAM_ML = \(d) lavaan::sam(model, data=d, sam.method = "global", estimator = "ML", std.lv= TRUE),
  GSAM_ULS = \(d) lavaan::sam(model, data=d, sam.method = "global", estimator = "ULS", std.lv= TRUE)
)
# postprocess each model output
models <- modify(models, ~compose(\(e)filter(e, label == "phi")$est, parameterEstimates, .))
# apply all estimators to the same dataset
apply_models <- \(d) map(models, exec, d)

planned_analysis <- compose(apply_models, simulate_data)

planned_analysis(100, 0, -.12)
```
# Extract results
```{r}

extract_results <- function(results_df){
#Compute performance measures
results_metrics <- results_df %>%
  group_by(N, rc, psi_value) %>%
  summarize(across(everything(),
                   list(
                     bias = ~mean(abs(.x - phi)),          # Average absolute bias
                     sd = ~sd(.x),                          # SD
                     rmse = ~sqrt(mean((.x - phi) ^ 2))  # Root mean square error
                   )), .groups = 'drop')
  split_metrics <- results_metrics %>%
    group_by(rc, psi_value) %>%
    group_split()

  # Define a function to transform each group into the desired format
  transform_group <- function(df_group) {
    df_group <- df_group %>%
                  select(-rc, -psi_value) %>%
                  pivot_longer(cols = starts_with(c("SEM_","LSAM_","GSAM_")), names_to = "method_metric", values_to = "value") 
        bias <- df_group %>% filter(str_detect(method_metric, "bias"))
        sd <- df_group %>% filter(str_detect(method_metric, "sd"))
        rmse <- df_group %>% filter(str_detect(method_metric, "rmse"))
        
  }

  # Apply the transformation to each group and store the results
  tables_list <- map(split_metrics, transform_group)

  return(tables_list)
}

tables_list <- extract_results(results_df)

```



#  Simulation Study

```{r}
# here a mechanism for the repetitions is missing
# ideally each repetition should be treated just like other design variables but with a random number seed
# take a look at withr (with_seed)
design <- setup_design()

simulation_study_ <- function(design){
  all_steps <- mutate(design, !!!future_pmap_dfr(design, planned_analysis, .options = furrr_options(seed = TRUE)))
  all_steps
}

simulation_study <- function(design, k, seed = NULL) {
  future_map_dfr(seq_len(k), ~simulation_study_(design), .options = furrr_options(seed = seed))
}

results_df <- simulation_study(design,2,1235)
results_df

```

# Report analysis
```{r}
report_analysis <- function(results_df, models) {
  # Define the list to store the tables
  tables_list <- list()

  # Define the different RC conditions and the corresponding labels
  unique_rc <- unique(results_df$RC)
  unique_psi <- unique(results_df$PsiValue)

  # Define metric names to iterate over
  metrics <- c("Bias", "SD", "RMSE")

  # Loop through each RC condition and PsiValue to create separate tables for each metric
  for (rc in unique_rc) {
    for (psi in unique_psi) {
      # Skip creation for non-existing conditions (RC == 0 and PsiValue is negative)
      if (rc == 0 && psi < 0) {
        next
      }
      
      # Filter for the current RC and PsiValue
      subset_df <- results_df %>%
        filter(RC == rc, PsiValue == psi) %>%
        mutate(ModelType = factor(ModelType, levels = models)) # Use the models object here

      # Loop through each metric to pivot and create tables
      for (metric in metrics) {
        metric_colnames <- grep(metric, names(subset_df), value = TRUE)
        
        # Pivot the dataframe to a wide format for the current metric and reorder rows
        wide_df <- subset_df %>%
          select(ModelType, N, all_of(metric_colnames)) %>%
          arrange(ModelType) %>% # Arrange rows by ModelType
          pivot_longer(cols = all_of(metric_colnames), names_to = "Metric", values_to = "Value") %>%
          mutate(Value = round(Value, 2)) %>% # Round the values
          pivot_wider(names_from = N, values_from = Value) %>%
          select(-Metric)
        
        # Generate the table name based on the condition and metric
        psi_label <- ifelse(psi == 0, "0", ifelse(psi > 0, "pos", "neg"))
        condition_label <- paste(rc, psi_label, "res", sep = "_")
        table_name <- paste(metric, condition_label, sep = "_")
        
        # Store the table in the list with the corrected naming
        tables_list[[table_name]] <- wide_df
      }
    }
  }
  
  return(tables_list)
}

list_of_tables <- report_analysis(results_df, models)

# Access a specific table
bias_no_res_table <- list_of_tables[["Bias_0_pos_res"]] # Example for Bias with no residual correlations

#display all tables
report_analysis(results_df, models)
```


# Questions
Open questions: 

- How exactly do we compute Bias, SD and RMSE for the estimated factor correlation?
Absolute vs. relative vs. mean bias in first studies?
```{r}
#Performance measures need to be extracted for each condition
#Below, just mathematical computation, does not work so commented out

#average_absolute_bias <- mean(abs(estimates - true_values))
#average_relative_bias <- mean((estimates - true_values) / true_values) 
```







